#include <cstdlib>
#include <cstring>
#include <fstream>
#include <iostream>
#include <iomanip>
#include <map>
#include <vector>
#include <memory>
#include <chrono>
#include <cassert>
#include <algorithm>
#include <cmath>
#include <cstring>

// TensorRT includes
#include "NvInfer.h"
#include "NvInferRuntime.h"
#include "cuda_runtime_api.h"

// CUDA é”™è¯¯æ£€æŸ¥å®
#define CHECK_CUDA(call) \
    do { \
        cudaError_t err = call; \
        if (err != cudaSuccess) { \
            std::cerr << "CUDA error at " << __FILE__ << ":" << __LINE__ << " - " << cudaGetErrorString(err) << std::endl; \
            exit(1); \
        } \
    } while(0)

// TensorRT é”™è¯¯æ£€æŸ¥å®
#define CHECK_TRT(call) \
    do { \
        if (!(call)) { \
            std::cerr << "TensorRT error at " << __FILE__ << ":" << __LINE__ << std::endl; \
            exit(1); \
        } \
    } while(0)

// ç®€å•çš„ Logger
class Logger : public nvinfer1::ILogger {
public:
    void log(Severity severity, const char* msg) noexcept override {
        if (severity <= Severity::kWARNING) {
            std::cout << "[TensorRT] " << msg << std::endl;
        }
    }
};

// Half ç²¾åº¦è½¬æ¢å‡½æ•°
float half2float(uint16_t h) {
    uint32_t sign = (h >> 15) & 0x1;
    uint32_t exponent = (h >> 10) & 0x1f;
    uint32_t mantissa = h & 0x3ff;

    if (exponent == 0) {
        if (mantissa == 0) {
            return sign ? -0.0f : 0.0f;
        } else {
            float f = mantissa / 1024.0f;
            f = f / 16384.0f;
            return sign ? -f : f;
        }
    } else if (exponent == 31) {
        if (mantissa == 0) {
            return sign ? -INFINITY : INFINITY;
        } else {
            return NAN;
        }
    }

    float f = 1.0f + (mantissa / 1024.0f);
    int exp = exponent - 15;
    f = f * std::pow(2.0f, exp);

    return sign ? -f : f;
}

class SimpleAlexNetInfer {
private:
    Logger logger;
    nvinfer1::IRuntime* runtime;
    nvinfer1::ICudaEngine* engine;
    nvinfer1::IExecutionContext* context;

    // ç½‘ç»œå‚æ•°
    static const int INPUT_N = 1;
    static const int INPUT_C = 3;
    static const int INPUT_H = 224;
    static const int INPUT_W = 224;
    static const int OUTPUT_SIZE = 1000;

    const std::string INPUT_BLOB_NAME = "alexnet_input";
    const std::string OUTPUT_BLOB_NAME = "alexnet_output";

    // æƒé‡æ˜ å°„
    std::map<std::string, std::vector<float>> weightMap;

    // GPU å†…å­˜
    void* inputBuffer = nullptr;
    void* outputBuffer = nullptr;
    void* buffers[2] = {nullptr, nullptr};

    // è¾“å…¥è¾“å‡ºå°ºå¯¸
    size_t inputSize;
    size_t outputSize;

public:
    SimpleAlexNetInfer() {
        // ä½¿ç”¨FP16è¾“å…¥ï¼Œæ‰€ä»¥æ˜¯2å­—èŠ‚è€Œä¸æ˜¯4å­—èŠ‚
        inputSize = INPUT_N * INPUT_C * INPUT_H * INPUT_W * sizeof(uint16_t);
        outputSize = OUTPUT_SIZE * sizeof(float);

        // åˆ†é… GPU å†…å­˜
        CHECK_CUDA(cudaMalloc(&inputBuffer, inputSize));
        CHECK_CUDA(cudaMalloc(&outputBuffer, outputSize));

        buffers[0] = inputBuffer;
        buffers[1] = outputBuffer;
    }

    ~SimpleAlexNetInfer() {
        if (inputBuffer) cudaFree(inputBuffer);
        if (outputBuffer) cudaFree(outputBuffer);
        if (context) context->destroy();
        if (engine) engine->destroy();
        if (runtime) runtime->destroy();
    }

    // åŠ è½½æƒé‡æ–‡ä»¶
    bool loadWeights(const std::string& filename) {
        std::cout << "åŠ è½½æƒé‡æ–‡ä»¶: " << filename << std::endl;

        std::ifstream input(filename);
        if (!input.is_open()) {
            std::cerr << "é”™è¯¯: æ— æ³•æ‰“å¼€æƒé‡æ–‡ä»¶ " << filename << std::endl;
            return false;
        }

        int32_t count;
        input >> count;

        if (count <= 0) {
            std::cerr << "é”™è¯¯: æ— æ•ˆçš„æƒé‡æ•°é‡ " << count << std::endl;
            return false;
        }

        std::cout << "åŠ è½½ " << count << " ä¸ªæƒé‡å±‚..." << std::endl;

        while (count--) {
            std::string name;
            uint32_t size;
            input >> name >> std::dec >> size;

            std::cout << "åŠ è½½å±‚: " << name << " (" << size << " å‚æ•°)" << std::endl;

            std::vector<float> weights(size);

            // è¯»å–åå…­è¿›åˆ¶æƒé‡æ•°æ®å¹¶è½¬æ¢ä¸ºfloat (ç»Ÿä¸€FP16æ ¼å¼)
            for (uint32_t i = 0; i < size; ++i) {
                uint32_t hex_val;
                input >> std::hex >> hex_val;

                // FP16æƒé‡ï¼šä»16ä½hexè½¬æ¢
                uint16_t fp16_val = static_cast<uint16_t>(hex_val);
                weights[i] = half2float(fp16_val);
            }

            weightMap[name] = std::move(weights);
        }

        std::cout << "æƒé‡åŠ è½½å®Œæˆ!" << std::endl;
        return true;
    }

    // æ„å»ºç½‘ç»œ
    bool buildNetwork() {
        std::cout << "æ„å»º AlexNet ç½‘ç»œ..." << std::endl;

        auto builder = nvinfer1::createInferBuilder(logger);
        if (!builder) {
            std::cerr << "é”™è¯¯: åˆ›å»º builder å¤±è´¥" << std::endl;
            return false;
        }

        const auto explicitBatch = 1U << static_cast<uint32_t>(nvinfer1::NetworkDefinitionCreationFlag::kEXPLICIT_BATCH);
        auto network = builder->createNetworkV2(explicitBatch);
        if (!network) {
            std::cerr << "é”™è¯¯: åˆ›å»º network å¤±è´¥" << std::endl;
            builder->destroy();
            return false;
        }

        // æ„å»ºç®€åŒ–çš„ AlexNet æ¶æ„
        if (!buildAlexNet(network)) {
            std::cerr << "é”™è¯¯: æ„å»º AlexNet å¤±è´¥" << std::endl;
            network->destroy();
            builder->destroy();
            return false;
        }

        // åˆ›å»ºå¼•æ“é…ç½®
        auto config = builder->createBuilderConfig();
        if (!config) {
            std::cerr << "é”™è¯¯: åˆ›å»º config å¤±è´¥" << std::endl;
            network->destroy();
            builder->destroy();
            return false;
        }

        // è®¾ç½®æœ€å¤§å·¥ä½œç©ºé—´å¤§å°
        config->setMaxWorkspaceSize(1U << 30); // 1GB

        // å¯ç”¨FP16ç²¾åº¦
        config->setFlag(nvinfer1::BuilderFlag::kFP16);
        std::cout << "å¯ç”¨ FP16 ç²¾åº¦æ¨ç†" << std::endl;

        // æ„å»ºå¼•æ“
        auto enginePlan = builder->buildSerializedNetwork(*network, *config);
        if (!enginePlan) {
            std::cerr << "é”™è¯¯: æ„å»ºå¼•æ“å¤±è´¥" << std::endl;
            config->destroy();
            network->destroy();
            builder->destroy();
            return false;
        }

        // åˆ›å»ºè¿è¡Œæ—¶
        runtime = nvinfer1::createInferRuntime(logger);
        if (!runtime) {
            std::cerr << "é”™è¯¯: åˆ›å»º runtime å¤±è´¥" << std::endl;
            return false;
        }

        // ååºåˆ—åŒ–å¼•æ“
        engine = runtime->deserializeCudaEngine(enginePlan->data(), enginePlan->size());
        if (!engine) {
            std::cerr << "é”™è¯¯: ååºåˆ—åŒ–å¼•æ“å¤±è´¥" << std::endl;
            return false;
        }

        // åˆ›å»ºæ‰§è¡Œä¸Šä¸‹æ–‡
        context = engine->createExecutionContext();
        if (!context) {
            std::cerr << "é”™è¯¯: åˆ›å»º execution context å¤±è´¥" << std::endl;
            return false;
        }

        // æ¸…ç†ä¸´æ—¶å¯¹è±¡
        enginePlan->destroy();
        config->destroy();
        network->destroy();
        builder->destroy();

        std::cout << "ç½‘ç»œæ„å»ºå®Œæˆ!" << std::endl;
        return true;
    }


    bool buildAlexNet(nvinfer1::INetworkDefinition* network) {
    

        auto input = network->addInput(INPUT_BLOB_NAME.c_str(), nvinfer1::DataType::kHALF,
                                     nvinfer1::Dims4{INPUT_N, INPUT_C, INPUT_H, INPUT_W});
        if (!input) {
            std::cerr << "é”™è¯¯: æ·»åŠ è¾“å…¥å±‚å¤±è´¥" << std::endl;
            return false;
        }
        std::cout << "âœ… åˆ›å»ºFP16è¾“å…¥å±‚: [1, 3, 224, 224] (NCHW)" << std::endl;

        // Conv1: features.0 - Conv2d(3, 64, kernel_size=11, stride=4, padding=2)
        std::cout << "æ·»åŠ  Conv1: 3->64, kernel=11, stride=4, pad=2" << std::endl;
        auto conv1 = addConvBNRelu(network, *input, 64, 11, 4, 2, "features.0", "features.1");
        if (!conv1) return false;

        // MaxPool1: features.2 - MaxPool2d(kernel_size=3, stride=2)
        std::cout << "æ·»åŠ  MaxPool1: kernel=3, stride=2" << std::endl;
        auto pool1 = network->addPoolingNd(*conv1, nvinfer1::PoolingType::kMAX, nvinfer1::DimsHW{3, 3});
        pool1->setStrideNd(nvinfer1::DimsHW{2, 2});

        // Conv2: features.3 - Conv2d(64, 192, kernel_size=5, padding=2)
        std::cout << "æ·»åŠ  Conv2: 64->192, kernel=5, stride=1, pad=2" << std::endl;
        auto conv2 = addConvBNRelu(network, *pool1->getOutput(0), 192, 5, 1, 2, "features.3", "features.4");
        if (!conv2) return false;

        // MaxPool2: features.5 - MaxPool2d(kernel_size=3, stride=2)
        std::cout << "æ·»åŠ  MaxPool2: kernel=3, stride=2" << std::endl;
        auto pool2 = network->addPoolingNd(*conv2, nvinfer1::PoolingType::kMAX, nvinfer1::DimsHW{3, 3});
        pool2->setStrideNd(nvinfer1::DimsHW{2, 2});

        // Conv3: features.6 - Conv2d(192, 384, kernel_size=3, padding=1)
        std::cout << "æ·»åŠ  Conv3: 192->384, kernel=3, stride=1, pad=1" << std::endl;
        auto conv3 = addConvRelu(network, *pool2->getOutput(0), 384, 3, 1, 1, "features.6");
        if (!conv3) return false;

        // Conv4: features.8 - Conv2d(384, 256, kernel_size=3, padding=1)
        std::cout << "æ·»åŠ  Conv4: 384->256, kernel=3, stride=1, pad=1" << std::endl;
        auto conv4 = addConvRelu(network, *conv3, 256, 3, 1, 1, "features.8");
        if (!conv4) return false;

        // Conv5: features.10 - Conv2d(256, 256, kernel_size=3, padding=1)
        std::cout << "æ·»åŠ  Conv5: 256->256, kernel=3, stride=1, pad=1" << std::endl;
        auto conv5 = addConvRelu(network, *conv4, 256, 3, 1, 1, "features.10");
        if (!conv5) return false;

        // MaxPool3: features.12 - MaxPool2d(kernel_size=3, stride=2)
        std::cout << "æ·»åŠ  MaxPool3: kernel=3, stride=2" << std::endl;
        auto pool3 = network->addPoolingNd(*conv5, nvinfer1::PoolingType::kMAX, nvinfer1::DimsHW{3, 3});
        pool3->setStrideNd(nvinfer1::DimsHW{2, 2});

        // Flatten - ä» [1, 256, 6, 6] å±•å¹³ä¸º [1, 9216]
        std::cout << "æ·»åŠ  Flatten: [1, 256, 6, 6] -> [1, 9216]" << std::endl;
        auto flatten = network->addShuffle(*pool3->getOutput(0));
        flatten->setReshapeDimensions(nvinfer1::Dims4{1, 256 * 6 * 6, 1, 1});

        // FC1: classifier.1 - Linear(9216, 4096)
        std::cout << "æ·»åŠ  FC1: 9216->4096" << std::endl;
        auto fc1 = addLinearRelu(network, *flatten->getOutput(0), 4096, "classifier.1");
        if (!fc1) return false;

        // FC2: classifier.4 - Linear(4096, 4096)
        std::cout << "æ·»åŠ  FC2: 4096->4096" << std::endl;
        auto fc2 = addLinearRelu(network, *fc1, 4096, "classifier.4");
        if (!fc2) return false;

        // FC3: classifier.6 - Linear(4096, 1000) - è¾“å‡ºå±‚
        std::cout << "æ·»åŠ  FC3: 4096->1000 (è¾“å‡ºå±‚)" << std::endl;
        auto fc3 = addLinear(network, *fc2, 1000, "classifier.6");
        if (!fc3) return false;

        auto softmax = network->addSoftMax(*fc3);
        if (!softmax) return false;

        // æ ‡è®°è¾“å‡º
        network->markOutput(*softmax->getOutput(0));
        softmax->getOutput(0)->setName(OUTPUT_BLOB_NAME.c_str());

        std::cout << "âœ… AlexNetç½‘ç»œæ¶æ„æ„å»ºå®Œæˆ!" << std::endl;
        return true;
    }

    nvinfer1::ITensor* addConvBNRelu(nvinfer1::INetworkDefinition* network, nvinfer1::ITensor& input,
                                   int outCh, int ksize, int stride, int pad,
                                   const std::string& convName, const std::string& bnName) {
        auto conv = addConv(network, input, outCh, ksize, stride, pad, convName);
        if (!conv) return nullptr;

        auto relu = network->addActivation(*conv, nvinfer1::ActivationType::kRELU);
        return relu->getOutput(0);
    }

    // æ·»åŠ å·ç§¯+ReLUå±‚
    nvinfer1::ITensor* addConvRelu(nvinfer1::INetworkDefinition* network, nvinfer1::ITensor& input,
                                 int outCh, int ksize, int stride, int pad, const std::string& name) {
        auto conv = addConv(network, input, outCh, ksize, stride, pad, name);
        if (!conv) return nullptr;

        auto relu = network->addActivation(*conv, nvinfer1::ActivationType::kRELU);
        return relu->getOutput(0);
    }

    // æ·»åŠ å·ç§¯å±‚
    nvinfer1::ITensor* addConv(nvinfer1::INetworkDefinition* network, nvinfer1::ITensor& input,
                             int outCh, int ksize, int stride, int pad, const std::string& name) {
        auto weightKey = name + ".weight";
        auto biasKey = name + ".bias";

        if (weightMap.find(weightKey) == weightMap.end() || weightMap.find(biasKey) == weightMap.end()) {
            std::cerr << "é”™è¯¯: æœªæ‰¾åˆ°æƒé‡ " << weightKey << " æˆ– " << biasKey << std::endl;
            return nullptr;
        }

        // int inCh = input.getDimensions().d[1];  // æœªä½¿ç”¨ï¼Œæ³¨é‡Šæ‰
        auto& weights = weightMap[weightKey];
        auto& bias = weightMap[biasKey];

        nvinfer1::Weights wt{nvinfer1::DataType::kFLOAT, weights.data(), (int64_t)weights.size()};
        nvinfer1::Weights bt{nvinfer1::DataType::kFLOAT, bias.data(), (int64_t)bias.size()};

        auto conv = network->addConvolutionNd(input, outCh, nvinfer1::DimsHW{ksize, ksize}, wt, bt);
        conv->setStrideNd(nvinfer1::DimsHW{stride, stride});
        conv->setPaddingNd(nvinfer1::DimsHW{pad, pad});

        return conv->getOutput(0);
    }

    // æ·»åŠ æ‰¹å½’ä¸€åŒ–å±‚
    nvinfer1::ITensor* addBatchNorm(nvinfer1::INetworkDefinition* network, nvinfer1::ITensor& input, const std::string& name) {
        auto weightKey = name + ".weight";
        auto biasKey = name + ".bias";
        auto meanKey = name + ".running_mean";
        auto varKey = name + ".running_var";

        if (weightMap.find(weightKey) == weightMap.end()) {
            return &input; // å¦‚æœæ²¡æœ‰BNæƒé‡ï¼Œç›´æ¥è¿”å›è¾“å…¥
        }

        auto& gamma = weightMap[weightKey];
        auto& beta = weightMap[biasKey];
        auto& mean = weightMap[meanKey];
        auto& var = weightMap[varKey];

        // è®¡ç®— scale å’Œ shift
        std::vector<float> scale(gamma.size());
        std::vector<float> shift(beta.size());
        const float eps = 1e-5;

        for (size_t i = 0; i < gamma.size(); ++i) {
            scale[i] = gamma[i] / std::sqrt(var[i] + eps);
            shift[i] = beta[i] - mean[i] * scale[i];
        }

        nvinfer1::Weights scaleWt{nvinfer1::DataType::kFLOAT, scale.data(), (int64_t)scale.size()};
        nvinfer1::Weights shiftWt{nvinfer1::DataType::kFLOAT, shift.data(), (int64_t)shift.size()};
        nvinfer1::Weights powerWt{nvinfer1::DataType::kFLOAT, nullptr, 0};

        auto bn = network->addScaleNd(input, nvinfer1::ScaleMode::kCHANNEL, shiftWt, scaleWt, powerWt, 1);
        return bn->getOutput(0);
    }

    // æ·»åŠ å…¨è¿æ¥+ReLUå±‚
    nvinfer1::ITensor* addLinearRelu(nvinfer1::INetworkDefinition* network, nvinfer1::ITensor& input,
                                   int outSize, const std::string& name) {
        auto fc = addLinear(network, input, outSize, name);
        if (!fc) return nullptr;

        auto relu = network->addActivation(*fc, nvinfer1::ActivationType::kRELU);
        return relu->getOutput(0);
    }

    // æ·»åŠ å…¨è¿æ¥å±‚
    nvinfer1::ITensor* addLinear(nvinfer1::INetworkDefinition* network, nvinfer1::ITensor& input,
                               int outSize, const std::string& name) {
        auto weightKey = name + ".weight";
        auto biasKey = name + ".bias";

        if (weightMap.find(weightKey) == weightMap.end() || weightMap.find(biasKey) == weightMap.end()) {
            std::cerr << "é”™è¯¯: æœªæ‰¾åˆ°æƒé‡ " << weightKey << " æˆ– " << biasKey << std::endl;
            return nullptr;
        }

        auto& weights = weightMap[weightKey];
        auto& bias = weightMap[biasKey];

        nvinfer1::Weights wt{nvinfer1::DataType::kFLOAT, weights.data(), (int64_t)weights.size()};
        nvinfer1::Weights bt{nvinfer1::DataType::kFLOAT, bias.data(), (int64_t)bias.size()};

        auto fc = network->addFullyConnected(input, outSize, wt, bt);
        return fc->getOutput(0);
    }

    // åŠ è½½è¾“å…¥æ•°æ® (æ”¯æŒFP16)
    bool loadInput(const std::string& filename) {
        std::cout << "åŠ è½½è¾“å…¥æ–‡ä»¶: " << filename << std::endl;

        std::ifstream file(filename, std::ios::binary);
        if (!file.is_open()) {
            std::cerr << "é”™è¯¯: æ— æ³•æ‰“å¼€è¾“å…¥æ–‡ä»¶ " << filename << std::endl;
            return false;
        }

        // æ ¹æ®æ–‡ä»¶ååˆ¤æ–­æ•°æ®ç±»å‹
        bool isFP16 = filename.find("fp16") != std::string::npos;

        if (isFP16) {
            // FP16è¾“å…¥æ•°æ®
            std::vector<uint16_t> inputDataFP16(INPUT_N * INPUT_C * INPUT_H * INPUT_W);
            file.read(reinterpret_cast<char*>(inputDataFP16.data()), inputSize);
            file.close();

            // å°†FP16æ•°æ®å¤åˆ¶åˆ°GPU
            CHECK_CUDA(cudaMemcpy(inputBuffer, inputDataFP16.data(), inputSize, cudaMemcpyHostToDevice));
            std::cout << "âœ… FP16è¾“å…¥æ•°æ®åŠ è½½å®Œæˆ!" << std::endl;
        } else {
            // FP32è¾“å…¥æ•°æ®ï¼Œéœ€è¦è½¬æ¢ä¸ºFP16
            std::vector<float> inputDataFP32(INPUT_N * INPUT_C * INPUT_H * INPUT_W);
            file.read(reinterpret_cast<char*>(inputDataFP32.data()), inputSize);
            file.close();

            // è½¬æ¢ä¸ºFP16
            std::vector<uint16_t> inputDataFP16(INPUT_N * INPUT_C * INPUT_H * INPUT_W);
            for (size_t i = 0; i < inputDataFP32.size(); ++i) {
                // ç®€å•çš„FP32åˆ°FP16è½¬æ¢
                float val = inputDataFP32[i];
                uint16_t fp16_val = 0;

                // æå–ç¬¦å·ä½
                uint32_t sign = (*(uint32_t*)&val) >> 31;
                uint32_t exp = ((*(uint32_t*)&val) >> 23) & 0xFF;
                uint32_t mantissa = (*(uint32_t*)&val) & 0x7FFFFF;

                if (exp == 0) {
                    fp16_val = sign << 15;
                } else if (exp == 255) {
                    fp16_val = (sign << 15) | 0x7C00;
                } else {
                    int new_exp = exp - 127 + 15;
                    if (new_exp <= 0) {
                        fp16_val = sign << 15;
                    } else if (new_exp >= 31) {
                        fp16_val = (sign << 15) | 0x7C00;
                    } else {
                        fp16_val = (sign << 15) | (new_exp << 10) | (mantissa >> 13);
                    }
                }

                inputDataFP16[i] = fp16_val;
            }

            // å°†FP16æ•°æ®å¤åˆ¶åˆ°GPU
            CHECK_CUDA(cudaMemcpy(inputBuffer, inputDataFP16.data(), inputSize, cudaMemcpyHostToDevice));
            std::cout << "âœ… FP32->FP16è¾“å…¥æ•°æ®è½¬æ¢å®Œæˆ!" << std::endl;
        }

        return true;
    }

    // æ‰§è¡Œæ¨ç†
    bool doInference(std::vector<float>& output) {
        std::cout << "æ‰§è¡Œæ¨ç†..." << std::endl;

        // æ‰§è¡Œæ¨ç†
        auto start = std::chrono::high_resolution_clock::now();
        bool success = context->executeV2(buffers);
        auto end = std::chrono::high_resolution_clock::now();

        if (!success) {
            std::cerr << "é”™è¯¯: æ¨ç†æ‰§è¡Œå¤±è´¥" << std::endl;
            return false;
        }

        auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end - start);
        std::cout << "æ¨ç†è€—æ—¶: " << duration.count() << " ms" << std::endl;

        // å°†ç»“æœä» GPU å¤åˆ¶åˆ° CPU
        output.resize(OUTPUT_SIZE);
        CHECK_CUDA(cudaMemcpy(output.data(), outputBuffer, outputSize, cudaMemcpyDeviceToHost));

        return true;
    }

    // æ€§èƒ½ç»Ÿè®¡ç»“æ„
    struct PerfStats {
        double min_time = 1e9;
        double max_time = 0.0;
        double total_time = 0.0;
        double avg_time = 0.0;
        int iterations = 0;
        bool valid = false;

        void addSample(double time_ms) {
            min_time = std::min(min_time, time_ms);
            max_time = std::max(max_time, time_ms);
            total_time += time_ms;
            iterations++;
            avg_time = total_time / iterations;
            valid = true;
        }

        void printStats(const std::string& name) {
            std::cout << "\n=== " << name << " æ€§èƒ½ç»Ÿè®¡ ===" << std::endl;
            if (!valid) {
                std::cout << "âš ï¸ æµ‹è¯•å¤±è´¥ï¼Œæ— æœ‰æ•ˆæ•°æ®" << std::endl;
                return;
            }
            std::cout << "æ‰§è¡Œæ¬¡æ•°: " << iterations << std::endl;
            std::cout << "å¹³å‡æ—¶é—´: " << std::fixed << std::setprecision(3) << avg_time << " ms" << std::endl;
            std::cout << "æœ€å°æ—¶é—´: " << std::fixed << std::setprecision(3) << min_time << " ms" << std::endl;
            std::cout << "æœ€å¤§æ—¶é—´: " << std::fixed << std::setprecision(3) << max_time << " ms" << std::endl;
            std::cout << "æ€»è®¡æ—¶é—´: " << std::fixed << std::setprecision(3) << total_time << " ms" << std::endl;
            std::cout << "ååé‡: " << std::fixed << std::setprecision(2) << (1000.0 / avg_time) << " FPS" << std::endl;
        }
    };

    // è®¡ç®—TOPSæ€§èƒ½æŒ‡æ ‡
    void calculateTOPS(const PerfStats& stats) {
        if (!stats.valid) {
            std::cout << "âš ï¸ æ— æ³•è®¡ç®—TOPSï¼Œæ€§èƒ½æ•°æ®æ— æ•ˆ" << std::endl;
            return;
        }

        // AlexNetæ¨¡å‹æ“ä½œæ•°ç»Ÿè®¡ (åŸºäºFP16ç²¾åº¦)
        const double alexnet_ops = 71600000.0;  // AlexNetæ€»æ“ä½œæ•°
        const double fp16_ops = alexnet_ops * 2;  // FP16éœ€è¦2å€æ“ä½œ

        // è½¬æ¢ä¸ºç§’
        double time_seconds = stats.avg_time / 1000.0;

        // è®¡ç®—å®é™…TOPS
        double actual_tops = fp16_ops / (time_seconds * 1e12);

        // è®¡ç®—ç†è®ºå³°å€¼TOPS (å‡è®¾GPUè§„æ ¼)
        double theoretical_peak = 0.0;
        std::string gpu_info = "æœªçŸ¥GPU";

        // å°è¯•è·å–GPUä¿¡æ¯
        cudaDeviceProp prop;
        if (cudaGetDeviceProperties(&prop, 0) == cudaSuccess) {
            gpu_info = prop.name;
            // åŸºäºGPUè§„æ ¼ä¼°ç®—ç†è®ºå³°å€¼
            if (strstr(prop.name, "RTX") || strstr(prop.name, "GTX")) {
                theoretical_peak = 10.0;  // å‡è®¾10 TOPSç†è®ºå³°å€¼
            } else if (strstr(prop.name, "Tesla")) {
                theoretical_peak = 20.0;  // å‡è®¾20 TOPSç†è®ºå³°å€¼
            } else {
                theoretical_peak = 5.0;   // é»˜è®¤5 TOPS
            }
        }

        std::cout << "\n=== TOPSæ€§èƒ½åˆ†æ ===" << std::endl;
        std::cout << "GPUè®¾å¤‡: " << gpu_info << std::endl;
        std::cout << "æ¨¡å‹: AlexNet (FP16ç²¾åº¦)" << std::endl;
        std::cout << "æ€»æ“ä½œæ•°: " << std::scientific << std::setprecision(2) << fp16_ops << std::endl;
        std::cout << "å¹³å‡æ¨ç†æ—¶é—´: " << std::fixed << std::setprecision(3) << stats.avg_time << " ms" << std::endl;
        std::cout << "å®é™…TOPS: " << std::fixed << std::setprecision(6) << actual_tops << " TOPS" << std::endl;
        std::cout << "ç†è®ºå³°å€¼: " << std::fixed << std::setprecision(2) << theoretical_peak << " TOPS" << std::endl;

        if (theoretical_peak > 0) {
            double efficiency = (actual_tops / theoretical_peak) * 100.0;
            std::cout << "ç¡¬ä»¶æ•ˆç‡: " << std::fixed << std::setprecision(2) << efficiency << "%" << std::endl;
        }

        // è®¡ç®—ä¸åŒç²¾åº¦ä¸‹çš„TOPS
        double int8_tops = actual_tops * 2.0;  // INT8ç²¾åº¦ä¸‹TOPSç¿»å€
        double fp32_tops = actual_tops * 0.5;  // FP32ç²¾åº¦ä¸‹TOPSå‡åŠ

        std::cout << "\n=== ä¸åŒç²¾åº¦TOPSé¢„ä¼° ===" << std::endl;
        std::cout << "INT8ç²¾åº¦é¢„ä¼°: " << std::fixed << std::setprecision(6) << int8_tops << " TOPS" << std::endl;
        std::cout << "FP16ç²¾åº¦å®æµ‹: " << std::fixed << std::setprecision(6) << actual_tops << " TOPS" << std::endl;
        std::cout << "FP32ç²¾åº¦é¢„ä¼°: " << std::fixed << std::setprecision(6) << fp32_tops << " TOPS" << std::endl;

        // æ€§èƒ½å»ºè®®
        std::cout << "\n=== æ€§èƒ½ä¼˜åŒ–å»ºè®® ===" << std::endl;
        if (actual_tops < 1.0) {
            std::cout << "ğŸ”§ å½“å‰TOPSè¾ƒä½ï¼Œå»ºè®®:" << std::endl;
            std::cout << "  - æ£€æŸ¥GPUåˆ©ç”¨ç‡æ˜¯å¦å……åˆ†" << std::endl;
            std::cout << "  - è€ƒè™‘ä½¿ç”¨INT8é‡åŒ–æå‡æ€§èƒ½" << std::endl;
            std::cout << "  - ä¼˜åŒ–å†…å­˜è®¿é—®æ¨¡å¼" << std::endl;
        } else if (actual_tops < 5.0) {
            std::cout << "âš¡ æ€§èƒ½ä¸­ç­‰ï¼Œå¯è¿›ä¸€æ­¥ä¼˜åŒ–:" << std::endl;
            std::cout << "  - å°è¯•TensorRTä¼˜åŒ–ç­–ç•¥" << std::endl;
            std::cout << "  - è€ƒè™‘æ¨¡å‹å‰ªææˆ–é‡åŒ–" << std::endl;
        } else {
            std::cout << "ğŸš€ æ€§èƒ½è‰¯å¥½!" << std::endl;
        }
    }

    // æ‰§è¡Œæ€§èƒ½æµ‹è¯•
    bool doPerformanceTest(int iterations = 100, int warmup = 10) {
        std::cout << "\n=== å¼€å§‹æ€§èƒ½æµ‹è¯• ===" << std::endl;
        std::cout << "é¢„çƒ­æ¬¡æ•°: " << warmup << std::endl;
        std::cout << "æµ‹è¯•æ¬¡æ•°: " << iterations << std::endl;

        PerfStats stats;

        // é¢„çƒ­é˜¶æ®µ
        std::cout << "é¢„çƒ­ä¸­..." << std::flush;
        for (int i = 0; i < warmup; ++i) {
            std::vector<float> dummy_output;
            if (!doInference(dummy_output)) {
                std::cerr << "é¢„çƒ­å¤±è´¥" << std::endl;
                return false;
            }
            if (i % 2 == 1) std::cout << "." << std::flush;
        }
        std::cout << " å®Œæˆ" << std::endl;

        // æ€§èƒ½æµ‹è¯•
        std::cout << "æ€§èƒ½æµ‹è¯•ä¸­..." << std::flush;
        for (int i = 0; i < iterations; ++i) {
            auto start = std::chrono::high_resolution_clock::now();

            std::vector<float> output;
            if (!doInference(output)) {
                std::cerr << "æ¨ç†å¤±è´¥ at iteration " << i << std::endl;
                return false;
            }

            auto end = std::chrono::high_resolution_clock::now();
            auto duration = std::chrono::duration_cast<std::chrono::microseconds>(end - start);
            double time_ms = duration.count() / 1000.0;
            stats.addSample(time_ms);

            if (i % 10 == 9) std::cout << "." << std::flush;
        }
        std::cout << " å®Œæˆ" << std::endl;

        // æ‰“å°æ€§èƒ½ç»Ÿè®¡
        stats.printStats("AlexNet FP16æ¨ç†");

        // è®¡ç®—TOPS
        calculateTOPS(stats);

        return true;
    }

    // æ‰“å°ç»“æœ
    void printResults(const std::vector<float>& output) {
        std::cout << "\n=== æ¨ç†ç»“æœ ===" << std::endl;

        // æ‰¾åˆ°æœ€å¤§å€¼çš„ç´¢å¼•
        auto maxIt = std::max_element(output.begin(), output.end());
        int maxIndex = std::distance(output.begin(), maxIt);

        std::cout << "é¢„æµ‹ç±»åˆ«: " << maxIndex << std::endl;
        std::cout << "ç½®ä¿¡åº¦: " << *maxIt << std::endl;

        // æ‰“å° Top 5 ç»“æœ
        std::vector<std::pair<float, int>> scoreIndex;
        for (int i = 0; i < OUTPUT_SIZE; ++i) {
            scoreIndex.push_back(std::make_pair(output[i], i));
        }
        std::sort(scoreIndex.begin(), scoreIndex.end(), std::greater<std::pair<float, int>>());

        std::cout << "\nTop 5 é¢„æµ‹ç»“æœ:" << std::endl;
        for (int i = 0; i < 5; ++i) {
            std::cout << "  " << (i+1) << ". ç±»åˆ« " << scoreIndex[i].second
                     << ": " << std::fixed << std::setprecision(6) << scoreIndex[i].first << std::endl;
        }
    }
};

int main() {
    std::cout << "=== AlexNet TensorRT å®Œæ•´FP16æ¨ç† + TOPSæ€§èƒ½åˆ†æ ===" << std::endl;
    std::cout << "âœ… åˆå§‹åŒ–FP16æ¨ç†å¼•æ“" << std::endl;
    std::cout << "  è¾“å…¥å¤§å°: " << (1 * 3 * 224 * 224 * sizeof(float)) << " bytes (" << (1 * 3 * 224 * 224 * sizeof(float) / 1024) << " KB)" << std::endl;
    std::cout << "  è¾“å‡ºå¤§å°: " << (1000 * sizeof(float)) << " bytes (" << (1000 * sizeof(float) / 1024) << " KB)" << std::endl;

    SimpleAlexNetInfer infer;

    // åŠ è½½FP16æƒé‡
    std::cout << "\nğŸ“ åŠ è½½FP16æƒé‡æ–‡ä»¶: alexnet-tensorrt-fp16.wts" << std::endl;
    if (!infer.loadWeights("alexnet-tensorrt-fp16.wts")) {
        std::cerr << "âŒ æƒé‡åŠ è½½å¤±è´¥" << std::endl;
        return -1;
    }
    std::cout << "âœ… æƒé‡åŠ è½½å®Œæˆ!" << std::endl;

    // æ„å»ºç½‘ç»œ
    std::cout << "\nğŸ”§ æ„å»º FP16 AlexNet ç½‘ç»œ..." << std::endl;
    if (!infer.buildNetwork()) {
        std::cerr << "âŒ ç½‘ç»œæ„å»ºå¤±è´¥" << std::endl;
        return -1;
    }
    std::cout << "âœ… FP16ç½‘ç»œæ„å»ºå®Œæˆ!" << std::endl;

    // åŠ è½½è¾“å…¥ - ä½¿ç”¨NCHWæ ¼å¼çš„è¾“å…¥æ–‡ä»¶
    std::cout << "\nğŸ“¥ åŠ è½½è¾“å…¥æ–‡ä»¶: input_fp32_nchw.bin" << std::endl;
    if (!infer.loadInput("input_fp32_nchw.bin")) {
        std::cerr << "âŒ è¾“å…¥åŠ è½½å¤±è´¥" << std::endl;
        return -1;
    }
    std::cout << "âœ… è¾“å…¥æ•°æ®åŠ è½½å®Œæˆ!" << std::endl;

    // æ‰§è¡Œå•æ¬¡æ¨ç†éªŒè¯
    std::cout << "\nğŸ” æ‰§è¡Œå•æ¬¡æ¨ç†éªŒè¯..." << std::endl;
    std::vector<float> output;
    if (!infer.doInference(output)) {
        std::cerr << "âŒ æ¨ç†æ‰§è¡Œå¤±è´¥" << std::endl;
        return -1;
    }

    // æ‰“å°æ¨ç†ç»“æœ
    std::cout << "\n=== æ¨ç†ç»“æœéªŒè¯ ===" << std::endl;
    infer.printResults(output);

    // æ‰§è¡Œæ€§èƒ½æµ‹è¯•å’ŒTOPSè®¡ç®—
    std::cout << "\nğŸš€ å¼€å§‹æ€§èƒ½æµ‹è¯•å’ŒTOPSåˆ†æ..." << std::endl;
    if (!infer.doPerformanceTest(100, 10)) {  // 100æ¬¡æµ‹è¯•ï¼Œ10æ¬¡é¢„çƒ­
        std::cerr << "âŒ æ€§èƒ½æµ‹è¯•å¤±è´¥" << std::endl;
        return -1;
    }

    // æ˜¾ç¤ºGPUä¿¡æ¯
    std::cout << "\n=== ç¡¬ä»¶ä¿¡æ¯ ===" << std::endl;
    cudaDeviceProp prop;
    if (cudaGetDeviceProperties(&prop, 0) == cudaSuccess) {
        std::cout << "GPUè®¾å¤‡: " << prop.name << std::endl;
        std::cout << "è®¡ç®—èƒ½åŠ›: " << prop.major << "." << prop.minor << std::endl;
        std::cout << "æ˜¾å­˜å¤§å°: " << (prop.totalGlobalMem / (1024 * 1024)) << " MB" << std::endl;
        std::cout << "å¤šå¤„ç†å™¨æ•°é‡: " << prop.multiProcessorCount << std::endl;
        std::cout << "æœ€å¤§çº¿ç¨‹æ•°: " << prop.maxThreadsPerBlock << std::endl;
    } else {
        std::cout << "âš ï¸ æ— æ³•è·å–GPUä¿¡æ¯" << std::endl;
    }

    std::cout << "\nğŸ‰ AlexNet FP16æ¨ç† + TOPSåˆ†æå®Œæˆ!" << std::endl;
    std::cout << "ğŸ“Š æ€§èƒ½åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆï¼Œè¯·æŸ¥çœ‹ä¸Šè¿°TOPSè®¡ç®—ç»“æœ" << std::endl;

    return 0;
}