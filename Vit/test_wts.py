#!/usr/bin/env python3
"""
ViT WTSæ–‡ä»¶éªŒè¯è„šæœ¬
éªŒè¯è½¬æ¢åçš„.wtsæ–‡ä»¶æ˜¯å¦å¯ä»¥æ­£ç¡®è¿›è¡Œå›¾ç‰‡æ¨ç†
"""

import torch
import torchvision.transforms as transforms
from PIL import Image
import os
import sys
import struct

def load_wts_weights(wts_file):
    """åŠ è½½.wtsæ–‡ä»¶ä¸­çš„æƒé‡"""
    print(f"ğŸ“¥ åŠ è½½.wtsæ–‡ä»¶: {wts_file}")
    
    weight_map = {}
    
    with open(wts_file, 'r') as f:
        # è¯»å–å¼ é‡æ•°é‡
        count = int(f.readline().strip())
        print(f"ğŸ“Š æ€»å¼ é‡æ•°é‡: {count}")
        
        for i in range(count):
            line = f.readline().strip()
            parts = line.split(' ')
            
            if len(parts) < 3:
                continue
                
            name = parts[0]
            size = int(parts[1])
            
            # è¯»å–æƒé‡æ•°æ®
            weights = []
            for j in range(2, len(parts)):
                if parts[j]:  # è·³è¿‡ç©ºå­—ç¬¦ä¸²
                    # å°†åå…­è¿›åˆ¶å­—ç¬¦ä¸²è½¬æ¢ä¸ºfloat
                    hex_str = parts[j]
                    if len(hex_str) == 8:  # ç¡®ä¿æ˜¯4å­—èŠ‚çš„åå…­è¿›åˆ¶
                        float_val = struct.unpack('>f', bytes.fromhex(hex_str))[0]
                        weights.append(float_val)
            
            if len(weights) == size:
                weight_map[name] = torch.tensor(weights, dtype=torch.float32)
                print(f"âœ… åŠ è½½å¼ é‡: {name}, å½¢çŠ¶: {len(weights)}")
            else:
                print(f"âŒ å¼ é‡å¤§å°ä¸åŒ¹é…: {name}, æœŸæœ›: {size}, å®é™…: {len(weights)}")
    
    print(f"ğŸ“¦ æˆåŠŸåŠ è½½ {len(weight_map)} ä¸ªå¼ é‡")
    return weight_map

def create_timm_vit():
    """åˆ›å»ºtimmå…¼å®¹çš„ViTæ¨¡å‹"""
    class PatchEmbed(torch.nn.Module):
        def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):
            super().__init__()
            self.img_size = img_size
            self.patch_size = patch_size
            self.num_patches = (img_size // patch_size) ** 2
            self.proj = torch.nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)
            
        def forward(self, x):
            B, C, H, W = x.shape
            x = self.proj(x).flatten(2).transpose(1, 2)
            return x

    class Attention(torch.nn.Module):
        def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):
            super().__init__()
            self.num_heads = num_heads
            head_dim = dim // num_heads
            self.scale = head_dim ** -0.5
            self.qkv = torch.nn.Linear(dim, dim * 3, bias=qkv_bias)
            self.attn_drop = torch.nn.Dropout(attn_drop)
            self.proj = torch.nn.Linear(dim, dim)
            self.proj_drop = torch.nn.Dropout(proj_drop)
            
        def forward(self, x):
            B, N, C = x.shape
            qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
            q, k, v = qkv[0], qkv[1], qkv[2]
            attn = (q @ k.transpose(-2, -1)) * self.scale
            attn = attn.softmax(dim=-1)
            attn = self.attn_drop(attn)
            x = (attn @ v).transpose(1, 2).reshape(B, N, C)
            x = self.proj(x)
            x = self.proj_drop(x)
            return x

    class Mlp(torch.nn.Module):
        def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=torch.nn.GELU, drop=0.):
            super().__init__()
            out_features = out_features or in_features
            hidden_features = hidden_features or in_features
            self.fc1 = torch.nn.Linear(in_features, hidden_features)
            self.act = act_layer()
            self.fc2 = torch.nn.Linear(hidden_features, out_features)
            self.drop = torch.nn.Dropout(drop)
            
        def forward(self, x):
            x = self.fc1(x)
            x = self.act(x)
            x = self.drop(x)
            x = self.fc2(x)
            x = self.drop(x)
            return x

    class Block(torch.nn.Module):
        def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0.):
            super().__init__()
            self.norm1 = torch.nn.LayerNorm(dim)
            self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)
            self.norm2 = torch.nn.LayerNorm(dim)
            mlp_hidden_dim = int(dim * mlp_ratio)
            self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=torch.nn.GELU, drop=drop)
            
        def forward(self, x):
            x = x + self.attn(self.norm1(x))
            x = x + self.mlp(self.norm2(x))
            return x

    class ViT(torch.nn.Module):
        def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4., qkv_bias=True, drop_rate=0., attn_drop_rate=0.):
            super().__init__()
            self.num_classes = num_classes
            self.num_features = self.embed_dim = embed_dim
            self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)
            num_patches = self.patch_embed.num_patches
            self.cls_token = torch.nn.Parameter(torch.zeros(1, 1, embed_dim))
            self.pos_embed = torch.nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))
            self.pos_drop = torch.nn.Dropout(p=drop_rate)
            self.blocks = torch.nn.Sequential(*[Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, drop=drop_rate, attn_drop=attn_drop_rate) for i in range(depth)])
            self.norm = torch.nn.LayerNorm(embed_dim)
            self.head = torch.nn.Linear(self.num_features, num_classes) if num_classes > 0 else torch.nn.Identity()
            
        def forward(self, x):
            B = x.shape[0]
            x = self.patch_embed(x)
            cls_tokens = self.cls_token.expand(B, -1, -1)
            x = torch.cat((cls_tokens, x), dim=1)
            x = x + self.pos_embed
            x = self.pos_drop(x)
            x = self.blocks(x)
            x = self.norm(x)
            x = self.head(x[:, 0])
            return x

    return ViT

def load_wts_to_model(model, weight_map, model_type):
    """å°†.wtsæƒé‡åŠ è½½åˆ°æ¨¡å‹ä¸­"""
    print(f"ğŸ”„ åŠ è½½æƒé‡åˆ°ViT-{model_type.upper()}æ¨¡å‹...")
    
    loaded_count = 0
    total_params = 0
    
    # ä½¿ç”¨ named_parameters() è·å–å®é™…å‚æ•°çš„å¼•ç”¨ï¼Œè€Œä¸æ˜¯å‰¯æœ¬
    for name, param in model.named_parameters():
        total_params += 1
        if name in weight_map:
            # è·å–æƒé‡å¼ é‡
            wts_tensor = weight_map[name]
            
            # é‡å¡‘å¼ é‡å½¢çŠ¶ä»¥åŒ¹é…æ¨¡å‹å‚æ•°
            if wts_tensor.numel() == param.numel():
                param.data = wts_tensor.view_as(param)
                loaded_count += 1
                print(f"âœ… åŠ è½½: {name}, å½¢çŠ¶: {param.shape}")
            else:
                print(f"âŒ å½¢çŠ¶ä¸åŒ¹é…: {name}, æ¨¡å‹: {param.shape}, WTS: {wts_tensor.shape}")
        else:
            print(f"âš ï¸  æœªæ‰¾åˆ°æƒé‡: {name}")
    
    # åŒæ—¶åŠ è½½ buffers (å¦‚ LayerNorm çš„ running stats)
    for name, buffer in model.named_buffers():
        if name in weight_map:
            wts_tensor = weight_map[name]
            if wts_tensor.numel() == buffer.numel():
                buffer.data = wts_tensor.view_as(buffer)
                loaded_count += 1
                print(f"âœ… åŠ è½½buffer: {name}, å½¢çŠ¶: {buffer.shape}")
    
    print(f"ğŸ“Š æˆåŠŸåŠ è½½ {loaded_count} ä¸ªå‚æ•°")
    return loaded_count > 0

def compare_with_original():
    """ä¸åŸå§‹PyTorchæ¨¡å‹æ¯”è¾ƒ"""
    print("ğŸ” ä¸åŸå§‹PyTorchæ¨¡å‹æ¯”è¾ƒ...")
    
    # åŠ è½½åŸå§‹æ¨¡å‹
    ViT = create_timm_vit()
    original_model = ViT(img_size=224, patch_size=16, num_classes=1000, 
                        embed_dim=768, depth=12, num_heads=12)
    
    # åŠ è½½åŸå§‹æƒé‡
    checkpoint = torch.load('../jx_vit_base_p16_224-80ecf9dd.pth', map_location='cpu', weights_only=True)
    original_model.load_state_dict(checkpoint)
    original_model.eval()
    
    # åŠ è½½WTSæƒé‡
    weight_map = load_wts_weights('vit_base.wts')
    wts_model = ViT(img_size=224, patch_size=16, num_classes=1000, 
                   embed_dim=768, depth=12, num_heads=12)
    load_wts_to_model(wts_model, weight_map, "base")
    wts_model.eval()
    
    # æµ‹è¯•å›¾ç‰‡
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    image = Image.open('../kitten.jpg').convert('RGB')
    input_tensor = transform(image).unsqueeze(0)
    
    # æ¯”è¾ƒè¾“å‡º
    with torch.no_grad():
        original_output = original_model(input_tensor)
        wts_output = wts_model(input_tensor)
        
        print(f"ğŸ“Š åŸå§‹æ¨¡å‹è¾“å‡ºå½¢çŠ¶: {original_output.shape}")
        print(f"ğŸ“Š WTSæ¨¡å‹è¾“å‡ºå½¢çŠ¶: {wts_output.shape}")
        
        # è®¡ç®—å·®å¼‚
        diff = torch.abs(original_output - wts_output)
        max_diff = torch.max(diff).item()
        mean_diff = torch.mean(diff).item()
        
        print(f"ğŸ” æœ€å¤§å·®å¼‚: {max_diff:.6f}")
        print(f"ğŸ” å¹³å‡å·®å¼‚: {mean_diff:.6f}")
        
        if max_diff < 1e-5:
            print("âœ… WTSæƒé‡ä¸åŸå§‹æƒé‡å®Œå…¨åŒ¹é…!")
        elif max_diff < 1e-3:
            print("âœ… WTSæƒé‡ä¸åŸå§‹æƒé‡åŸºæœ¬åŒ¹é…!")
        else:
            print("âŒ WTSæƒé‡ä¸åŸå§‹æƒé‡å­˜åœ¨è¾ƒå¤§å·®å¼‚!")
            
        # æ˜¾ç¤ºTop5é¢„æµ‹
        original_probs = torch.softmax(original_output, dim=1)
        wts_probs = torch.softmax(wts_output, dim=1)
        
        original_top5 = torch.topk(original_probs, 5)
        wts_top5 = torch.topk(wts_probs, 5)
        
        print("\nğŸ¯ åŸå§‹æ¨¡å‹Top5é¢„æµ‹:")
        for i, (prob, idx) in enumerate(zip(original_top5[0][0], original_top5[1][0])):
            print(f"   {i+1}. ç±»åˆ« {idx.item():3d}: {prob.item():.4f} ({prob.item()*100:.2f}%)")
            
        print("\nğŸ¯ WTSæ¨¡å‹Top5é¢„æµ‹:")
        for i, (prob, idx) in enumerate(zip(wts_top5[0][0], wts_top5[1][0])):
            print(f"   {i+1}. ç±»åˆ« {idx.item():3d}: {prob.item():.4f} ({prob.item()*100:.2f}%)")

def test_inference(model, image_path):
    """æµ‹è¯•æ¨¡å‹æ¨ç†"""
    print(f"ğŸ–¼ï¸  æµ‹è¯•å›¾ç‰‡: {image_path}")
    
    # æ£€æŸ¥å›¾ç‰‡æ˜¯å¦å­˜åœ¨
    if not os.path.exists(image_path):
        print(f"âŒ é”™è¯¯: å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
        return False
    
    # å›¾åƒé¢„å¤„ç†
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    # åŠ è½½å’Œé¢„å¤„ç†å›¾ç‰‡
    image = Image.open(image_path).convert('RGB')
    input_tensor = transform(image).unsqueeze(0)
    
    print(f"ğŸ“ è¾“å…¥å½¢çŠ¶: {input_tensor.shape}")
    
    # æ¨ç†
    model.eval()
    with torch.no_grad():
        output = model(input_tensor)
        probabilities = torch.softmax(output, dim=1)
        top5_prob, top5_indices = torch.topk(probabilities, 5)
    
    print(f"ğŸ“Š è¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"ğŸ¯ Top 5 é¢„æµ‹ç»“æœ:")
    for i, (prob, idx) in enumerate(zip(top5_prob[0], top5_indices[0])):
        print(f"   {i+1}. ç±»åˆ« {idx.item():3d}: {prob.item():.4f} ({prob.item()*100:.2f}%)")
    
    return True

def main():
    print("=" * 60)
    print("ViT WTSæ–‡ä»¶éªŒè¯æµ‹è¯•")
    print("=" * 60)
    
    # æ£€æŸ¥å‚æ•°
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python3 test_wts.py <wts_file> [model_type] [image_path]")
        print("ç¤ºä¾‹: python3 test_wts.py vit_base.wts base kitten.jpg")
        print("æˆ–è€…: python3 test_wts.py compare  # ä¸åŸå§‹æ¨¡å‹æ¯”è¾ƒ")
        return
    
    if sys.argv[1] == "compare":
        # ä¸åŸå§‹æ¨¡å‹æ¯”è¾ƒ
        compare_with_original()
        return
    
    wts_file = sys.argv[1]
    model_type = sys.argv[2] if len(sys.argv) > 2 else "base"
    image_path = sys.argv[3] if len(sys.argv) > 3 else "kitten.jpg"
    
    print(f"ğŸ“ WTSæ–‡ä»¶: {wts_file}")
    print(f"ğŸ—ï¸  æ¨¡å‹ç±»å‹: ViT-{model_type.upper()}")
    print(f"ğŸ–¼ï¸  æµ‹è¯•å›¾ç‰‡: {image_path}")
    print()
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(wts_file):
        print(f"âŒ é”™è¯¯: WTSæ–‡ä»¶ä¸å­˜åœ¨: {wts_file}")
        return
    
    # åŠ è½½.wtsæƒé‡
    weight_map = load_wts_weights(wts_file)
    print()
    
    # åˆ›å»ºæ¨¡å‹
    print("ğŸ—ï¸  åˆ›å»ºViTæ¨¡å‹...")
    ViT = create_timm_vit()
    
    if model_type.lower() == "large":
        model = ViT(img_size=224, patch_size=16, num_classes=1000, 
                   embed_dim=1024, depth=24, num_heads=16)
    else:  # base
        model = ViT(img_size=224, patch_size=16, num_classes=1000, 
                   embed_dim=768, depth=12, num_heads=12)
    
    print(f"ğŸ“Š æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    print()
    
    # åŠ è½½æƒé‡åˆ°æ¨¡å‹
    success = load_wts_to_model(model, weight_map, model_type)
    print()
    
    if not success:
        print("âŒ æƒé‡åŠ è½½å¤±è´¥!")
        return
    
    # æµ‹è¯•æ¨ç†
    print("ğŸš€ å¼€å§‹æ¨ç†æµ‹è¯•...")
    test_inference(model, image_path)
    
    print()
    print("=" * 60)
    print("âœ… WTSæ–‡ä»¶éªŒè¯å®Œæˆ!")
    print("=" * 60)

if __name__ == "__main__":
    main()
