#include <cstdlib>
#include <cstring>
#include <fstream>
#include <iostream>
#include <iomanip>
#include <map>
#include <vector>
#include <memory>
#include <chrono>
#include <cassert>
#include <algorithm>
#include <cmath>
#include <cstring>

// TensorRT includes
#include "NvInfer.h"
#include "NvInferRuntime.h"
#include "cuda_runtime_api.h"

// CUDA é”™è¯¯æ£€æŸ¥å®
#define CHECK_CUDA(call) \
    do { \
        cudaError_t err = call; \
        if (err != cudaSuccess) { \
            std::cerr << "CUDA error at " << __FILE__ << ":" << __LINE__ << " - " << cudaGetErrorString(err) << std::endl; \
            exit(1); \
        } \
    } while(0)

// TensorRT é”™è¯¯æ£€æŸ¥å®
#define CHECK_TRT(call) \
    do { \
        if (!(call)) { \
            std::cerr << "TensorRT error at " << __FILE__ << ":" << __LINE__ << std::endl; \
            exit(1); \
        } \
    } while(0)

// ç®€å•çš„ Logger
class Logger : public nvinfer1::ILogger {
public:
    void log(Severity severity, const char* msg) noexcept override {
        if (severity <= Severity::kWARNING) {
            std::cout << "[TensorRT] " << msg << std::endl;
        }
    }
};

// Half ç²¾åº¦è½¬æ¢å‡½æ•°
float half2float(uint16_t h) {
    uint32_t sign = (h >> 15) & 0x1;
    uint32_t exponent = (h >> 10) & 0x1f;
    uint32_t mantissa = h & 0x3ff;

    if (exponent == 0) {
        if (mantissa == 0) {
            return sign ? -0.0f : 0.0f;
        } else {
            float f = mantissa / 1024.0f;
            f = f / 16384.0f;
            return sign ? -f : f;
        }
    } else if (exponent == 31) {
        if (mantissa == 0) {
            return sign ? -INFINITY : INFINITY;
        } else {
            return NAN;
        }
    }

    float f = 1.0f + (mantissa / 1024.0f);
    int exp = exponent - 15;
    f = f * std::pow(2.0f, exp);

    return sign ? -f : f;
}

class ResNet50TensorRT {
private:
    Logger logger;
    nvinfer1::IRuntime* runtime;
    nvinfer1::ICudaEngine* engine;
    nvinfer1::IExecutionContext* context;
    nvinfer1::IBuilder* builder;
    nvinfer1::INetworkDefinition* network;

    // ç½‘ç»œå‚æ•° - å›ºå®šBatch=16, ä½¿ç”¨FP16
    static const int INPUT_N = 16;  // å›ºå®šbatch size
    static const int INPUT_C = 3;
    static const int INPUT_H = 224;
    static const int INPUT_W = 224;
    static const int OUTPUT_SIZE = 1000;
    
    // ä½¿ç”¨FP16æ•°æ®ç±»å‹
    static const nvinfer1::DataType INPUT_DATA_TYPE = nvinfer1::DataType::kHALF;
    static const nvinfer1::DataType OUTPUT_DATA_TYPE = nvinfer1::DataType::kHALF;

    const std::string INPUT_BLOB_NAME = "resnet50_input";
    const std::string OUTPUT_BLOB_NAME = "resnet50_output";

    // æƒé‡æ˜ å°„ - ä½¿ç”¨FP16
    std::map<std::string, std::vector<uint16_t>> weightMap;

    // è½¬æ¢åçš„floatæƒé‡å­˜å‚¨ - ç¡®ä¿Weightså¯¹è±¡çš„ç”Ÿå‘½å‘¨æœŸ
    std::map<std::string, std::vector<float>> floatWeightMap;

    // GPU å†…å­˜
    void* inputBuffer = nullptr;
    void* outputBuffer = nullptr;

public:
    ResNet50TensorRT() : runtime(nullptr), engine(nullptr), context(nullptr), builder(nullptr), network(nullptr) {
        // åˆå§‹åŒ–CUDA
        CHECK_CUDA(cudaSetDevice(0));
        
        // åˆ›å»ºTensorRTè¿è¡Œæ—¶
        runtime = nvinfer1::createInferRuntime(logger);
        CHECK_TRT(runtime != nullptr);
        
        // åˆ›å»ºæ„å»ºå™¨
        builder = nvinfer1::createInferBuilder(logger);
        CHECK_TRT(builder != nullptr);
        
        // åˆ›å»ºç½‘ç»œå®šä¹‰
        network = builder->createNetworkV2(1U << static_cast<uint32_t>(nvinfer1::NetworkDefinitionCreationFlag::kEXPLICIT_BATCH));
        CHECK_TRT(network != nullptr);
    }

    ~ResNet50TensorRT() {
        // æ¸…ç†GPUå†…å­˜
        if (inputBuffer) {
            cudaFree(inputBuffer);
            inputBuffer = nullptr;
        }
        if (outputBuffer) {
            cudaFree(outputBuffer);
            outputBuffer = nullptr;
        }
        
        // æ¸…ç†TensorRTå¯¹è±¡
        if (context) {
            context->destroy();
            context = nullptr;
        }
        if (engine) {
            engine->destroy();
            engine = nullptr;
        }
        if (runtime) {
            runtime->destroy();
            runtime = nullptr;
        }
        if (network) {
            network->destroy();
            network = nullptr;
        }
        if (builder) {
            builder->destroy();
            builder = nullptr;
        }
    }

    // åŠ è½½FP16æƒé‡
    void LoadWeights(const std::string& file) {
        std::cout << "Loading FP16 weights: " << file << std::endl;

        std::ifstream input(file);
        if (!input.is_open()) {
            std::cerr << "ERROR: Unable to load weight file: " << file << std::endl;
            exit(1);
        }

        // è¯»å–æƒé‡æ•°é‡
        int32_t count;
        input >> count;
        if (count <= 0) {
            std::cerr << "ERROR: Invalid weight count: " << count << std::endl;
            exit(1);
        }

        std::cout << "Loading " << count << " weight tensors..." << std::endl;

        while (count--) {
            std::string name;
            uint32_t size;
            input >> name >> std::dec >> size;

            // åŠ è½½FP16æƒé‡
            std::vector<uint16_t> val(size);
            for (uint32_t x = 0; x < size; ++x) {
                uint32_t temp;
                input >> std::hex >> temp;
                val[x] = static_cast<uint16_t>(temp & 0xFFFF);
            }

            weightMap[name] = val;
        }

        std::cout << "Successfully loaded " << weightMap.size() << " weight tensors" << std::endl;
    }

    // åˆ›å»ºå·ç§¯å±‚ï¼ˆä¸å¸¦biasï¼ŒResNetæ ‡å‡†å®ç°ï¼‰
    nvinfer1::IConvolutionLayer* AddConvLayer(nvinfer1::ITensor* input,
                                             const std::string& weightName,
                                             int outChannels, int kernelSize,
                                             nvinfer1::DimsHW stride, nvinfer1::DimsHW padding) {
        // è·å–æƒé‡
        auto weightIt = weightMap.find(weightName);

        if (weightIt == weightMap.end()) {
            std::cerr << "ERROR: Weight not found: " << weightName << std::endl;
            exit(1);
        }

        // å°†FP16æƒé‡è½¬æ¢ä¸ºfloatç”¨äºTensorRTï¼Œå¹¶å­˜å‚¨åœ¨mapä¸­ä»¥ç¡®ä¿ç”Ÿå‘½å‘¨æœŸ
        std::string weightKey = weightName + "_float";

        // åªåœ¨ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶è½¬æ¢å¹¶å­˜å‚¨
        if (floatWeightMap.find(weightKey) == floatWeightMap.end()) {
            std::vector<float> floatWeights;
            for (auto w : weightIt->second) {
                floatWeights.push_back(half2float(w));
            }
            floatWeightMap[weightKey] = std::move(floatWeights);
        }

        // åˆ›å»ºWeightså¯¹è±¡ï¼Œä½¿ç”¨å­˜å‚¨åœ¨mapä¸­çš„æ•°æ®
        nvinfer1::Weights wt{nvinfer1::DataType::kFLOAT, floatWeightMap[weightKey].data(), static_cast<int64_t>(floatWeightMap[weightKey].size())};

        // ResNetçš„Convå±‚æ²¡æœ‰biasï¼Œä½¿ç”¨ç©ºçš„Weights
        nvinfer1::Weights emptyBias{nvinfer1::DataType::kFLOAT, nullptr, 0};

        // åˆ›å»ºå·ç§¯å±‚
        auto conv = network->addConvolutionNd(*input, outChannels, nvinfer1::DimsHW{kernelSize, kernelSize}, wt, emptyBias);
        conv->setStrideNd(stride);
        conv->setPaddingNd(padding);
        conv->setName(("conv_" + weightName).c_str());

        return conv;
    }

    // åˆ›å»ºBatchNormå±‚
    nvinfer1::IScaleLayer* AddBatchNormLayer(nvinfer1::ITensor* input,
                                            const std::string& weightName,
                                            const std::string& biasName,
                                            const std::string& meanName,
                                            const std::string& varName,
                                            float epsilon = 1e-5) {
        // è·å–å‚æ•°
        auto weightIt = weightMap.find(weightName);
        auto biasIt = weightMap.find(biasName);
        auto meanIt = weightMap.find(meanName);
        auto varIt = weightMap.find(varName);

        if (weightIt == weightMap.end() || biasIt == weightMap.end() ||
            meanIt == weightMap.end() || varIt == weightMap.end()) {
            std::cerr << "ERROR: BatchNorm parameters not found" << std::endl;
            exit(1);
        }

        // ç”Ÿæˆå­˜å‚¨é”®
        std::string scaleKey = weightName + "_scale";
        std::string shiftKey = weightName + "_shift";

        // åªåœ¨ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶è½¬æ¢å¹¶è®¡ç®—
        if (floatWeightMap.find(scaleKey) == floatWeightMap.end()) {
            // å°†FP16å‚æ•°è½¬æ¢ä¸ºfloat
            std::vector<float> gamma, beta, mean, var;
            for (auto w : weightIt->second) gamma.push_back(half2float(w));
            for (auto b : biasIt->second) beta.push_back(half2float(b));
            for (auto m : meanIt->second) mean.push_back(half2float(m));
            for (auto v : varIt->second) var.push_back(half2float(v));

            // è®¡ç®— scale å’Œ shift: y = scale * x + shift
            // BatchNormå…¬å¼: y = gamma * (x - mean) / sqrt(var + eps) + beta
            // è½¬æ¢ä¸º: y = (gamma / sqrt(var + eps)) * x + (beta - mean * gamma / sqrt(var + eps))
            std::vector<float> scale(gamma.size());
            std::vector<float> shift(beta.size());

            for (size_t i = 0; i < gamma.size(); ++i) {
                scale[i] = gamma[i] / std::sqrt(var[i] + epsilon);
                shift[i] = beta[i] - mean[i] * scale[i];
            }

            // å­˜å‚¨è®¡ç®—ç»“æœ
            floatWeightMap[scaleKey] = std::move(scale);
            floatWeightMap[shiftKey] = std::move(shift);
        }

        // åˆ›å»ºWeightså¯¹è±¡ï¼Œä½¿ç”¨å­˜å‚¨åœ¨mapä¸­çš„æ•°æ®
        nvinfer1::Weights scaleWt{nvinfer1::DataType::kFLOAT, floatWeightMap[scaleKey].data(), static_cast<int64_t>(floatWeightMap[scaleKey].size())};
        nvinfer1::Weights shiftWt{nvinfer1::DataType::kFLOAT, floatWeightMap[shiftKey].data(), static_cast<int64_t>(floatWeightMap[shiftKey].size())};
        nvinfer1::Weights powerWt{nvinfer1::DataType::kFLOAT, nullptr, 0};

        // åˆ›å»ºScaleå±‚å®ç°BatchNorm
        auto scaleLayer = network->addScaleNd(*input, nvinfer1::ScaleMode::kCHANNEL, shiftWt, scaleWt, powerWt, 1);
        scaleLayer->setName(("bn_" + weightName).c_str());

        return scaleLayer;
    }

    // åˆ›å»ºBottleneckå—
    nvinfer1::ITensor* AddBottleneckBlock(nvinfer1::ITensor* input,
                                        const std::string& prefix,
                                        int inChannels, int outChannels,
                                        int stride = 1) {
        // conv1: 1x1å·ç§¯
        auto conv1 = AddConvLayer(input, prefix + "conv1.weight",
                                outChannels, 1, nvinfer1::DimsHW{1, 1}, nvinfer1::DimsHW{0, 0});
        auto bn1 = AddBatchNormLayer(conv1->getOutput(0), prefix + "bn1.weight", prefix + "bn1.bias",
                                   prefix + "bn1.running_mean", prefix + "bn1.running_var");
        auto relu1 = network->addActivation(*bn1->getOutput(0), nvinfer1::ActivationType::kRELU);

        // conv2: 3x3å·ç§¯
        auto conv2 = AddConvLayer(relu1->getOutput(0), prefix + "conv2.weight",
                                outChannels, 3, nvinfer1::DimsHW{stride, stride}, nvinfer1::DimsHW{1, 1});
        auto bn2 = AddBatchNormLayer(conv2->getOutput(0), prefix + "bn2.weight", prefix + "bn2.bias",
                                   prefix + "bn2.running_mean", prefix + "bn2.running_var");
        auto relu2 = network->addActivation(*bn2->getOutput(0), nvinfer1::ActivationType::kRELU);

        // conv3: 1x1å·ç§¯
        auto conv3 = AddConvLayer(relu2->getOutput(0), prefix + "conv3.weight",
                                outChannels * 4, 1, nvinfer1::DimsHW{1, 1}, nvinfer1::DimsHW{0, 0});
        auto bn3 = AddBatchNormLayer(conv3->getOutput(0), prefix + "bn3.weight", prefix + "bn3.bias",
                                   prefix + "bn3.running_mean", prefix + "bn3.running_var");

        // æ®‹å·®è¿æ¥
        nvinfer1::ITensor* residual = input;
        if (stride != 1 || inChannels != outChannels * 4) {
            // ä¸‹é‡‡æ ·å±‚
            auto downsample_conv = AddConvLayer(input, prefix + "downsample.0.weight",
                                              outChannels * 4, 1, nvinfer1::DimsHW{stride, stride}, nvinfer1::DimsHW{0, 0});
            auto downsample_bn = AddBatchNormLayer(downsample_conv->getOutput(0), prefix + "downsample.1.weight", prefix + "downsample.1.bias",
                                                 prefix + "downsample.1.running_mean", prefix + "downsample.1.running_var");
            residual = downsample_bn->getOutput(0);
        }

        // ç›¸åŠ 
        auto add = network->addElementWise(*residual, *bn3->getOutput(0), nvinfer1::ElementWiseOperation::kSUM);
        auto relu3 = network->addActivation(*add->getOutput(0), nvinfer1::ActivationType::kRELU);

        return relu3->getOutput(0);
    }

    // æ„å»ºResNet50ç½‘ç»œ
    void BuildResNet50() {
        std::cout << "Building ResNet50 network..." << std::endl;

        // è¾“å…¥å±‚ - ä½¿ç”¨FP16
        auto input = network->addInput(INPUT_BLOB_NAME.c_str(), INPUT_DATA_TYPE, 
                                      nvinfer1::Dims4{INPUT_N, INPUT_C, INPUT_H, INPUT_W});

        // conv1: 7x7å·ç§¯ï¼Œstride=2
        auto conv1 = AddConvLayer(input, "conv1.weight", 64, 7,
                                 nvinfer1::DimsHW{2, 2}, nvinfer1::DimsHW{3, 3});
        auto bn1 = AddBatchNormLayer(conv1->getOutput(0), "bn1.weight", "bn1.bias",
                                   "bn1.running_mean", "bn1.running_var");
        auto relu1 = network->addActivation(*bn1->getOutput(0), nvinfer1::ActivationType::kRELU);

        // maxpool: 3x3, stride=2
        auto maxpool = network->addPoolingNd(*relu1->getOutput(0), nvinfer1::PoolingType::kMAX, 
                                           nvinfer1::DimsHW{3, 3});
        maxpool->setStrideNd(nvinfer1::DimsHW{2, 2});
        maxpool->setPaddingNd(nvinfer1::DimsHW{1, 1});

        // layer1: 3ä¸ªbottleneckå—
        auto x = AddBottleneckBlock(maxpool->getOutput(0), "layer1.0.", 64, 64, 1);
        x = AddBottleneckBlock(x, "layer1.1.", 256, 64, 1);
        x = AddBottleneckBlock(x, "layer1.2.", 256, 64, 1);

        // layer2: 4ä¸ªbottleneckå—ï¼Œç¬¬ä¸€ä¸ªstride=2
        x = AddBottleneckBlock(x, "layer2.0.", 256, 128, 2);
        x = AddBottleneckBlock(x, "layer2.1.", 512, 128, 1);
        x = AddBottleneckBlock(x, "layer2.2.", 512, 128, 1);
        x = AddBottleneckBlock(x, "layer2.3.", 512, 128, 1);

        // layer3: 6ä¸ªbottleneckå—ï¼Œç¬¬ä¸€ä¸ªstride=2
        x = AddBottleneckBlock(x, "layer3.0.", 512, 256, 2);
        x = AddBottleneckBlock(x, "layer3.1.", 1024, 256, 1);
        x = AddBottleneckBlock(x, "layer3.2.", 1024, 256, 1);
        x = AddBottleneckBlock(x, "layer3.3.", 1024, 256, 1);
        x = AddBottleneckBlock(x, "layer3.4.", 1024, 256, 1);
        x = AddBottleneckBlock(x, "layer3.5.", 1024, 256, 1);

        // layer4: 3ä¸ªbottleneckå—ï¼Œç¬¬ä¸€ä¸ªstride=2
        x = AddBottleneckBlock(x, "layer4.0.", 1024, 512, 2);
        x = AddBottleneckBlock(x, "layer4.1.", 2048, 512, 1);
        x = AddBottleneckBlock(x, "layer4.2.", 2048, 512, 1);

        // å…¨å±€å¹³å‡æ± åŒ– (Global Average Poolingï¼Œè¾“å‡º1x1)
        auto avgpool = network->addPoolingNd(*x, nvinfer1::PoolingType::kAVERAGE,
                                           nvinfer1::DimsHW{7, 7});
        avgpool->setStrideNd(nvinfer1::DimsHW{1, 1});  // è®¾ç½®strideä¸º1ï¼Œä½¿kernelè¦†ç›–æ•´ä¸ª7x7åŒºåŸŸ

        // å…¨è¿æ¥å±‚ - å°†æƒé‡å­˜å‚¨åœ¨mapä¸­ä»¥ç¡®ä¿ç”Ÿå‘½å‘¨æœŸ
        std::string fc_weight_key = "fc.weight_float";
        std::string fc_bias_key = "fc.bias_float";

        if (floatWeightMap.find(fc_weight_key) == floatWeightMap.end()) {
            auto fc_weight = weightMap["fc.weight"];
            std::vector<float> fc_weight_float;
            for (auto w : fc_weight) fc_weight_float.push_back(half2float(w));
            floatWeightMap[fc_weight_key] = std::move(fc_weight_float);
        }

        if (floatWeightMap.find(fc_bias_key) == floatWeightMap.end()) {
            auto fc_bias = weightMap["fc.bias"];
            std::vector<float> fc_bias_float;
            for (auto b : fc_bias) fc_bias_float.push_back(half2float(b));
            floatWeightMap[fc_bias_key] = std::move(fc_bias_float);
        }

        nvinfer1::Weights fc_w{nvinfer1::DataType::kFLOAT, floatWeightMap[fc_weight_key].data(), static_cast<int64_t>(floatWeightMap[fc_weight_key].size())};
        nvinfer1::Weights fc_b{nvinfer1::DataType::kFLOAT, floatWeightMap[fc_bias_key].data(), static_cast<int64_t>(floatWeightMap[fc_bias_key].size())};

        auto fc = network->addFullyConnected(*avgpool->getOutput(0), OUTPUT_SIZE, fc_w, fc_b);
        fc->setName("fc");

        // æ·»åŠ Softmaxå±‚
        auto softmax = network->addSoftMax(*fc->getOutput(0));
        softmax->setName("softmax");

        // æ ‡è®°è¾“å‡º - ä½¿ç”¨FP16
        softmax->getOutput(0)->setName(OUTPUT_BLOB_NAME.c_str());
        softmax->getOutput(0)->setType(OUTPUT_DATA_TYPE);
        network->markOutput(*softmax->getOutput(0));

        std::cout << "ResNet50 network built successfully!" << std::endl;
    }

    // æ„å»ºå¼•æ“
    void BuildEngine() {
        std::cout << "Building TensorRT engine..." << std::endl;

        // åˆ›å»ºæ„å»ºé…ç½®
        auto config = builder->createBuilderConfig();
        config->setMaxWorkspaceSize(1 << 30); // 1GB
        config->setFlag(nvinfer1::BuilderFlag::kFP16); // å¯ç”¨FP16

        // æ„å»ºå¼•æ“
        engine = builder->buildEngineWithConfig(*network, *config);
        CHECK_TRT(engine != nullptr);

        // åˆ›å»ºæ‰§è¡Œä¸Šä¸‹æ–‡
        context = engine->createExecutionContext();
        CHECK_TRT(context != nullptr);

        std::cout << "TensorRT engine built successfully!" << std::endl;

        // æ¸…ç†
        config->destroy();
    }

    // åˆ†é…GPUå†…å­˜
    void AllocateBuffers() {
        std::cout << "Allocating GPU memory..." << std::endl;

        // åˆ†é…è¾“å…¥ç¼“å†²åŒº - FP16
        size_t inputSize = INPUT_N * INPUT_C * INPUT_H * INPUT_W * sizeof(uint16_t);
        CHECK_CUDA(cudaMalloc(&inputBuffer, inputSize));

        // åˆ†é…è¾“å‡ºç¼“å†²åŒº - FP16
        size_t outputSize = INPUT_N * OUTPUT_SIZE * sizeof(uint16_t);
        CHECK_CUDA(cudaMalloc(&outputBuffer, outputSize));

        std::cout << "GPU memory allocated successfully!" << std::endl;
    }

    // æ¨ç†
    void Infer(const std::string& inputFile, const std::string& outputFile) {
        std::cout << "Running inference..." << std::endl;

        // åŠ è½½è¾“å…¥æ•°æ®
        std::ifstream input(inputFile, std::ios::binary);
        if (!input) {
            std::cerr << "ERROR: Cannot open input file: " << inputFile << std::endl;
            exit(1);
        }

        size_t inputSize = INPUT_N * INPUT_C * INPUT_H * INPUT_W * sizeof(float);
        input.read(reinterpret_cast<char*>(inputBuffer), inputSize);
        input.close();

        // æ‰§è¡Œæ¨ç†
        void* bindings[] = {inputBuffer, outputBuffer};
        bool status = context->executeV2(bindings);
        if (!status) {
            std::cerr << "ERROR: Inference execution failed!" << std::endl;
            exit(1);
        }

        // ä¿å­˜è¾“å‡º
        std::ofstream output(outputFile, std::ios::binary);
        if (!output) {
            std::cerr << "ERROR: Cannot create output file: " << outputFile << std::endl;
            exit(1);
        }

        size_t outputSize = INPUT_N * OUTPUT_SIZE * sizeof(float);
        output.write(reinterpret_cast<char*>(outputBuffer), outputSize);
        output.close();

        std::cout << "Inference completed successfully!" << std::endl;
        std::cout << "Output saved to: " << outputFile << std::endl;
    }

    // æ€§èƒ½ç»Ÿè®¡ç»“æ„
    struct PerfStats {
        double min_time = 1e9;
        double max_time = 0.0;
        double total_time = 0.0;
        double avg_time = 0.0;
        int iterations = 0;
        bool valid = false;

        void addSample(double time_ms) {
            min_time = std::min(min_time, time_ms);
            max_time = std::max(max_time, time_ms);
            total_time += time_ms;
            iterations++;
            avg_time = total_time / iterations;
            valid = true;
        }

        void printStats(const std::string& name) {
            std::cout << "\n=== " << name << " æ€§èƒ½ç»Ÿè®¡ ===" << std::endl;
            if (!valid) {
                std::cout << "âš ï¸ æµ‹è¯•å¤±è´¥ï¼Œæ— æœ‰æ•ˆæ•°æ®" << std::endl;
                return;
            }
            std::cout << "æ‰§è¡Œæ¬¡æ•°: " << iterations << std::endl;
            std::cout << "å¹³å‡æ—¶é—´: " << std::fixed << std::setprecision(3) << avg_time << " ms" << std::endl;
            std::cout << "æœ€å°æ—¶é—´: " << std::fixed << std::setprecision(3) << min_time << " ms" << std::endl;
            std::cout << "æœ€å¤§æ—¶é—´: " << std::fixed << std::setprecision(3) << max_time << " ms" << std::endl;
            std::cout << "æ€»è®¡æ—¶é—´: " << std::fixed << std::setprecision(3) << total_time << " ms" << std::endl;
            std::cout << "ååé‡: " << std::fixed << std::setprecision(2) << (1000.0 / avg_time) * INPUT_N << " FPS" << std::endl;
        }
    };

    // è®¡ç®—TOPSæ€§èƒ½æŒ‡æ ‡ (ä¸mytestCodefork/main.cppä¿æŒä¸€è‡´)
    void CalculateTOPS(const PerfStats& stats) {
        if (!stats.valid) {
            std::cout << "âš ï¸ æ— æ³•è®¡ç®—TOPSï¼Œæ€§èƒ½æ•°æ®æ— æ•ˆ" << std::endl;
            return;
        }

        // ResNet50æ¨¡å‹æ“ä½œæ•°ç»Ÿè®¡ (åŸºäºFP16ç²¾åº¦ï¼ŒBatch=16)
        // ResNet50å¤§çº¦æœ‰ 4.1 GFLOPS (multiply-adds counted as 2 operations)
        const double resnet50_ops = 4100000000.0;  // ResNet50æ€»æ“ä½œæ•° (çº¦4.1 GFLOPS)
        const double fp16_ops = resnet50_ops * 2;  // FP16éœ€è¦2å€æ“ä½œ
        const double batch_fp16_ops = fp16_ops * INPUT_N;  // Batch=16çš„æ€»æ“ä½œæ•°

        // è½¬æ¢ä¸ºç§’ (ä½¿ç”¨çº¯æ¨ç†æ—¶é—´ï¼Œä¸mytestCodefork/main.cppä¿æŒä¸€è‡´)
        double time_seconds = stats.avg_time / 1000.0;

        // è®¡ç®—å®é™…TOPS (åŸºäºçº¯æ¨ç†æ—¶é—´ï¼Œæ›´å…¬å¹³)
        double actual_tops = batch_fp16_ops / (time_seconds * 1e12);

        // è®¡ç®—ç†è®ºå³°å€¼TOPS (å‡è®¾GPUè§„æ ¼)
        double theoretical_peak = 0.0;
        std::string gpu_info = "æœªçŸ¥GPU";

        // å°è¯•è·å–GPUä¿¡æ¯
        cudaDeviceProp prop;
        if (cudaGetDeviceProperties(&prop, 0) == cudaSuccess) {
            gpu_info = prop.name;
            // åŸºäºGPUè§„æ ¼ä¼°ç®—ç†è®ºå³°å€¼
            if (strstr(prop.name, "RTX") || strstr(prop.name, "GTX")) {
                theoretical_peak = 10.0;  // å‡è®¾10 TOPSç†è®ºå³°å€¼
            } else if (strstr(prop.name, "Tesla")) {
                theoretical_peak = 20.0;  // å‡è®¾20 TOPSç†è®ºå³°å€¼
            } else if (strstr(prop.name, "Orin")) {
                theoretical_peak = 5.0;   // Jetson Orinçº¦5 TOPS
            } else {
                theoretical_peak = 5.0;   // é»˜è®¤5 TOPS
            }
        }

        std::cout << "\n=== TOPSæ€§èƒ½åˆ†æ (åŸºäºçº¯æ¨ç†æ—¶é—´) ===" << std::endl;
        std::cout << "GPUè®¾å¤‡: " << gpu_info << std::endl;
        std::cout << "æ¨¡å‹: ResNet50 (FP16ç²¾åº¦, Batch=" << INPUT_N << ")" << std::endl;
        std::cout << "å•æ¬¡æ“ä½œæ•°: " << std::scientific << std::setprecision(2) << fp16_ops << std::endl;
        std::cout << "æ‰¹å¤„ç†æ“ä½œæ•°: " << std::scientific << std::setprecision(2) << batch_fp16_ops << std::endl;
        std::cout << "å¹³å‡æ¨ç†æ—¶é—´: " << std::fixed << std::setprecision(3) << stats.avg_time << " ms" << std::endl;
        std::cout << "å®é™…TOPS (çº¯æ¨ç†): " << std::fixed << std::setprecision(6) << actual_tops << " TOPS" << std::endl;
        std::cout << "ç†è®ºå³°å€¼: " << std::fixed << std::setprecision(2) << theoretical_peak << " TOPS" << std::endl;

        if (theoretical_peak > 0) {
            double efficiency = (actual_tops / theoretical_peak) * 100.0;
            std::cout << "ç¡¬ä»¶æ•ˆç‡: " << std::fixed << std::setprecision(2) << efficiency << "%" << std::endl;
        }

        // è®¡ç®—ä¸åŒç²¾åº¦ä¸‹çš„TOPS
        double int8_tops = actual_tops * 2.0;  // INT8ç²¾åº¦ä¸‹TOPSç¿»å€
        double fp32_tops = actual_tops * 0.5;  // FP32ç²¾åº¦ä¸‹TOPSå‡åŠ
        
        // è®¡ç®—å•å¼ å›¾ç‰‡ç­‰æ•ˆTOPS
        double per_image_tops = actual_tops / INPUT_N;

        std::cout << "\n=== ä¸åŒç²¾åº¦TOPSé¢„ä¼° ===" << std::endl;
        std::cout << "INT8ç²¾åº¦é¢„ä¼°: " << std::fixed << std::setprecision(6) << int8_tops << " TOPS" << std::endl;
        std::cout << "FP16ç²¾åº¦å®æµ‹: " << std::fixed << std::setprecision(6) << actual_tops << " TOPS" << std::endl;
        std::cout << "FP32ç²¾åº¦é¢„ä¼°: " << std::fixed << std::setprecision(6) << fp32_tops << " TOPS" << std::endl;
        std::cout << "å•å¼ å›¾ç‰‡ç­‰æ•ˆTOPS: " << std::fixed << std::setprecision(6) << per_image_tops << " TOPS" << std::endl;

        // æ€§èƒ½å»ºè®®
        std::cout << "\n=== æ€§èƒ½ä¼˜åŒ–å»ºè®® ===" << std::endl;
        if (actual_tops < 1.0) {
            std::cout << "ğŸ”§ å½“å‰TOPSè¾ƒä½ï¼Œå»ºè®®:" << std::endl;
            std::cout << "  - æ£€æŸ¥GPUåˆ©ç”¨ç‡æ˜¯å¦å……åˆ†" << std::endl;
            std::cout << "  - è€ƒè™‘ä½¿ç”¨INT8é‡åŒ–æå‡æ€§èƒ½" << std::endl;
            std::cout << "  - ä¼˜åŒ–å†…å­˜è®¿é—®æ¨¡å¼" << std::endl;
        } else if (actual_tops < 5.0) {
            std::cout << "âš¡ æ€§èƒ½ä¸­ç­‰ï¼Œå¯è¿›ä¸€æ­¥ä¼˜åŒ–:" << std::endl;
            std::cout << "  - å°è¯•TensorRTä¼˜åŒ–ç­–ç•¥" << std::endl;
            std::cout << "  - è€ƒè™‘æ¨¡å‹å‰ªææˆ–é‡åŒ–" << std::endl;
        } else {
            std::cout << "ğŸš€ æ€§èƒ½è‰¯å¥½!" << std::endl;
        }
    }

    // æ‰§è¡Œå•æ¬¡æ¨ç†å¹¶è¿”å›ç»“æœ (Batch=16, FP16)
    bool DoInference(std::vector<float>& output) {
        // æ£€æŸ¥ç¼“å†²åŒºæ˜¯å¦æœ‰æ•ˆ
        if (!inputBuffer || !outputBuffer) {
            std::cerr << "ERROR: GPU buffers not allocated" << std::endl;
            return false;
        }
        
        // æ‰§è¡Œæ¨ç†
        void* bindings[] = {inputBuffer, outputBuffer};
        bool status = context->executeV2(bindings);
        if (!status) {
            std::cerr << "ERROR: Inference execution failed" << std::endl;
            return false;
        }

        // å°†FP16ç»“æœä» GPU å¤åˆ¶åˆ° CPU
        std::vector<uint16_t> outputFP16(INPUT_N * OUTPUT_SIZE);
        size_t outputSize = INPUT_N * OUTPUT_SIZE * sizeof(uint16_t);
        
        cudaError_t err = cudaMemcpy(outputFP16.data(), outputBuffer, outputSize, cudaMemcpyDeviceToHost);
        if (err != cudaSuccess) {
            std::cerr << "ERROR: cudaMemcpy failed: " << cudaGetErrorString(err) << std::endl;
            return false;
        }

        // å°†FP16è½¬æ¢ä¸ºFP32ç”¨äºæ˜¾ç¤º
        output.resize(INPUT_N * OUTPUT_SIZE);
        for (size_t i = 0; i < output.size(); ++i) {
            output[i] = half2float(outputFP16[i]);
        }

        return true;
    }

    // æ‰§è¡Œçº¯æ¨ç†ï¼ˆä¸åŒ…å«å†…å­˜æ‹·è´ï¼‰
    bool DoInferenceOnly() {
        // æ£€æŸ¥ç¼“å†²åŒºæ˜¯å¦æœ‰æ•ˆ
        if (!inputBuffer || !outputBuffer) {
            std::cerr << "ERROR: GPU buffers not allocated" << std::endl;
            return false;
        }
        
        // æ‰§è¡Œæ¨ç†
        void* bindings[] = {inputBuffer, outputBuffer};
        bool status = context->executeV2(bindings);
        if (!status) {
            std::cerr << "ERROR: Inference execution failed" << std::endl;
            return false;
        }
        return true;
    }

    // æ‰§è¡Œæ€§èƒ½æµ‹è¯•
    bool DoPerformanceTest(int iterations = 100, int warmup = 10) {
        std::cout << "\n=== å¼€å§‹æ€§èƒ½æµ‹è¯• ===" << std::endl;
        std::cout << "é¢„çƒ­æ¬¡æ•°: " << warmup << std::endl;
        std::cout << "æµ‹è¯•æ¬¡æ•°: " << iterations << std::endl;

        PerfStats inference_stats;  // çº¯æ¨ç†ç»Ÿè®¡
        PerfStats total_stats;      // æ€»æ‰§è¡Œç»Ÿè®¡
        PerfStats memory_stats;     // å†…å­˜æ‹·è´ç»Ÿè®¡

        // é¢„çƒ­é˜¶æ®µ
        std::cout << "é¢„çƒ­ä¸­..." << std::flush;
        for (int i = 0; i < warmup; ++i) {
            std::vector<float> dummy_output;
            if (!DoInference(dummy_output)) {
                std::cerr << "é¢„çƒ­å¤±è´¥" << std::endl;
                return false;
            }
            // ç¡®ä¿CUDAæ“ä½œå®Œæˆ
            cudaDeviceSynchronize();
            if (i % 2 == 1) std::cout << "." << std::flush;
        }
        std::cout << " å®Œæˆ" << std::endl;

        // é¢„åˆ†é…è¾“å‡ºç¼“å†²åŒºï¼Œé¿å…é‡å¤åˆ†é… (FP16)
        std::vector<uint16_t> output_buffer_fp16(INPUT_N * OUTPUT_SIZE);
        size_t outputSize = INPUT_N * OUTPUT_SIZE * sizeof(uint16_t);
        
        // æ€§èƒ½æµ‹è¯•
        std::cout << "æ€§èƒ½æµ‹è¯•ä¸­..." << std::flush;
        for (int i = 0; i < iterations; ++i) {
            // æµ‹è¯•æ€»æ‰§è¡Œæ—¶é—´ï¼ˆæ¨ç†+å†…å­˜æ‹·è´ï¼‰
            auto total_start = std::chrono::high_resolution_clock::now();
            
            // æµ‹è¯•çº¯æ¨ç†æ—¶é—´
            auto inference_start = std::chrono::high_resolution_clock::now();
            if (!DoInferenceOnly()) {
                std::cerr << "æ¨ç†å¤±è´¥ at iteration " << i << std::endl;
                return false;
            }
            auto inference_end = std::chrono::high_resolution_clock::now();
            
            // æµ‹è¯•å†…å­˜æ‹·è´æ—¶é—´ (FP16)
            auto memory_start = std::chrono::high_resolution_clock::now();
            CHECK_CUDA(cudaMemcpy(output_buffer_fp16.data(), outputBuffer, outputSize, cudaMemcpyDeviceToHost));
            auto memory_end = std::chrono::high_resolution_clock::now();
            
            auto total_end = std::chrono::high_resolution_clock::now();

            // è®¡ç®—å„é˜¶æ®µæ—¶é—´
            auto inference_duration = std::chrono::duration_cast<std::chrono::microseconds>(inference_end - inference_start);
            auto memory_duration = std::chrono::duration_cast<std::chrono::microseconds>(memory_end - memory_start);
            auto total_duration = std::chrono::duration_cast<std::chrono::microseconds>(total_end - total_start);

            double inference_time_ms = inference_duration.count() / 1000.0;
            double memory_time_ms = memory_duration.count() / 1000.0;
            double total_time_ms = total_duration.count() / 1000.0;

            inference_stats.addSample(inference_time_ms);
            memory_stats.addSample(memory_time_ms);
            total_stats.addSample(total_time_ms);

            if (i % 10 == 9) std::cout << "." << std::flush;
        }
        std::cout << " å®Œæˆ" << std::endl;

        // è¯¦ç»†æ€§èƒ½ç»Ÿè®¡è¾“å‡º
        std::cout << "\n=== Inference Only æ€§èƒ½ç»Ÿè®¡ ===" << std::endl;
        std::cout << "Iterations: " << inference_stats.iterations << std::endl;
        std::cout << "Average time: " << std::fixed << std::setprecision(3) << inference_stats.avg_time << " ms" << std::endl;
        std::cout << "Min time: " << std::fixed << std::setprecision(3) << inference_stats.min_time << " ms" << std::endl;
        std::cout << "Max time: " << std::fixed << std::setprecision(3) << inference_stats.max_time << " ms" << std::endl;
        std::cout << "Total time: " << std::fixed << std::setprecision(3) << inference_stats.total_time << " ms" << std::endl;
        if (inference_stats.avg_time > 0) {
            double fps = (1000.0 / inference_stats.avg_time) * INPUT_N;
            std::cout << "Throughput: " << std::fixed << std::setprecision(2) << fps << " FPS" << std::endl;
        }
        std::cout << "=============================" << std::endl;

        std::cout << "\n=== Memory Copy (D2H) æ€§èƒ½ç»Ÿè®¡ ===" << std::endl;
        std::cout << "Iterations: " << memory_stats.iterations << std::endl;
        std::cout << "Average time: " << std::fixed << std::setprecision(3) << memory_stats.avg_time << " ms" << std::endl;
        std::cout << "Min time: " << std::fixed << std::setprecision(3) << memory_stats.min_time << " ms" << std::endl;
        std::cout << "Max time: " << std::fixed << std::setprecision(3) << memory_stats.max_time << " ms" << std::endl;
        std::cout << "Total time: " << std::fixed << std::setprecision(3) << memory_stats.total_time << " ms" << std::endl;
        if (memory_stats.avg_time > 0) {
            double fps = (1000.0 / memory_stats.avg_time) * INPUT_N;
            std::cout << "Throughput: " << std::fixed << std::setprecision(2) << fps << " FPS" << std::endl;
        }
        std::cout << "=============================" << std::endl;

        std::cout << "\n=== Total Execution æ€§èƒ½ç»Ÿè®¡ ===" << std::endl;
        std::cout << "Iterations: " << total_stats.iterations << std::endl;
        std::cout << "Average time: " << std::fixed << std::setprecision(3) << total_stats.avg_time << " ms" << std::endl;
        std::cout << "Min time: " << std::fixed << std::setprecision(3) << total_stats.min_time << " ms" << std::endl;
        std::cout << "Max time: " << std::fixed << std::setprecision(3) << total_stats.max_time << " ms" << std::endl;
        std::cout << "Total time: " << std::fixed << std::setprecision(3) << total_stats.total_time << " ms" << std::endl;
        if (total_stats.avg_time > 0) {
            double fps = (1000.0 / total_stats.avg_time) * INPUT_N;
            std::cout << "Throughput: " << std::fixed << std::setprecision(2) << fps << " FPS" << std::endl;
        }
        std::cout << "=============================" << std::endl;

        // è®¡ç®—TOPSï¼ˆåŸºäºçº¯æ¨ç†æ—¶é—´ï¼‰
        CalculateTOPS(inference_stats);

        // é¢å¤–æ€§èƒ½åˆ†æ
        std::cout << "\n=== æ•ˆç‡åˆ†æ ===" << std::endl;
        double inference_ratio = (inference_stats.avg_time / total_stats.avg_time) * 100.0;
        double memory_ratio = (memory_stats.avg_time / total_stats.avg_time) * 100.0;
        std::cout << "æ¨ç†æ—¶é—´å æ¯”: " << std::fixed << std::setprecision(2) << inference_ratio << "%" << std::endl;
        std::cout << "å†…å­˜æ‹·è´å æ¯”: " << std::fixed << std::setprecision(2) << memory_ratio << "%" << std::endl;
        
        // è®¡ç®—æ¯å¼ å›¾ç‰‡çš„å¤„ç†æ—¶é—´ (åŸºäºçº¯æ¨ç†æ—¶é—´ï¼Œæ›´å…¬å¹³)
        double per_image_inference_time = inference_stats.avg_time / INPUT_N;
        double per_image_total_time = total_stats.avg_time / INPUT_N;
        std::cout << "å•å¼ å›¾ç‰‡æ¨ç†æ—¶é—´: " << std::fixed << std::setprecision(3) << per_image_inference_time << " ms" << std::endl;
        std::cout << "å•å¼ å›¾ç‰‡æ€»æ—¶é—´: " << std::fixed << std::setprecision(3) << per_image_total_time << " ms" << std::endl;
        
        // è®¡ç®—ç†è®ºæœ€å¤§ååé‡ (åŸºäºçº¯æ¨ç†æ—¶é—´ï¼Œä¸mytestCodefork/main.cppä¿æŒä¸€è‡´)
        double theoretical_max_fps = (1000.0 / inference_stats.avg_time) * INPUT_N;
        std::cout << "ç†è®ºæœ€å¤§ååé‡ (çº¯æ¨ç†): " << std::fixed << std::setprecision(2) << theoretical_max_fps << " FPS" << std::endl;
        
        // è®¡ç®—å®é™…ç«¯åˆ°ç«¯ååé‡
        double end_to_end_fps = (1000.0 / total_stats.avg_time) * INPUT_N;
        std::cout << "ç«¯åˆ°ç«¯ååé‡ (å«å†…å­˜æ‹·è´): " << std::fixed << std::setprecision(2) << end_to_end_fps << " FPS" << std::endl;

        // å†…å­˜å¸¦å®½åˆ†æ (FP16)
        size_t input_size = INPUT_N * INPUT_C * INPUT_H * INPUT_W * sizeof(uint16_t);
        size_t output_size = INPUT_N * OUTPUT_SIZE * sizeof(uint16_t);
        double input_mb = input_size / (1024.0 * 1024.0);
        double output_mb = output_size / (1024.0 * 1024.0);
        
        std::cout << "\n=== å†…å­˜åˆ†æ ===" << std::endl;
        std::cout << "è¾“å…¥å¤§å°: " << std::fixed << std::setprecision(2) << input_mb << " MB" << std::endl;
        std::cout << "è¾“å‡ºå¤§å°: " << std::fixed << std::setprecision(2) << output_mb << " MB" << std::endl;
        
        if (memory_stats.avg_time > 0) {
            double memory_bandwidth = (output_mb / (memory_stats.avg_time / 1000.0));
            std::cout << "å†…å­˜å¸¦å®½ (D2H): " << std::fixed << std::setprecision(2) << memory_bandwidth << " MB/s" << std::endl;
        }
        
        double total_data_mb = input_mb + output_mb;
        std::cout << "å¤„ç†æ•°æ®æ€»é‡: " << std::fixed << std::setprecision(2) << total_data_mb << " MB" << std::endl;
        std::cout << "æ€»å¤„ç†æ•°æ®é‡: " << std::fixed << std::setprecision(2) << total_data_mb * inference_stats.iterations << " MB" << std::endl;

        // æ¸…ç†ä¸´æ—¶ç¼“å†²åŒº
        output_buffer_fp16.clear();
        output_buffer_fp16.shrink_to_fit();

        return true;
    }

    // æ‰“å°ç»“æœ (Batch=16)
    void PrintResults(const std::vector<float>& output) {
        std::cout << "\n=== ResNet50 Batch=" << INPUT_N << " æ¨ç†ç»“æœ ===" << std::endl;

        // ä¸ºæ¯ä¸ªæ ·æœ¬æ˜¾ç¤ºç»“æœ
        for (int batch_idx = 0; batch_idx < INPUT_N; ++batch_idx) {
            std::cout << "\n--- æ ·æœ¬ " << (batch_idx + 1) << "/" << INPUT_N << " ---" << std::endl;
            
            // è·å–å½“å‰æ ·æœ¬çš„è¾“å‡º
            std::vector<float> sample_output(OUTPUT_SIZE);
            for (int i = 0; i < OUTPUT_SIZE; ++i) {
                sample_output[i] = output[batch_idx * OUTPUT_SIZE + i];
            }

            // æ‰¾åˆ°æœ€å¤§å€¼çš„ç´¢å¼•
            auto maxIt = std::max_element(sample_output.begin(), sample_output.end());
            int maxIndex = std::distance(sample_output.begin(), maxIt);

            std::cout << "é¢„æµ‹ç±»åˆ«: " << maxIndex << std::endl;
            std::cout << "ç½®ä¿¡åº¦: " << std::fixed << std::setprecision(6) << *maxIt << std::endl;

            // æ‰“å° Top 5 ç»“æœ
            std::vector<std::pair<float, int>> scoreIndex;
            for (int i = 0; i < OUTPUT_SIZE; ++i) {
                scoreIndex.push_back(std::make_pair(sample_output[i], i));
            }
            std::sort(scoreIndex.begin(), scoreIndex.end(), std::greater<std::pair<float, int>>());

            std::cout << "Top 5 é¢„æµ‹ç»“æœ:" << std::endl;
            for (int i = 0; i < 5; ++i) {
                std::cout << "  " << (i+1) << ". ç±»åˆ« " << scoreIndex[i].second
                         << ": " << std::fixed << std::setprecision(6) << scoreIndex[i].first << std::endl;
            }
        }
    }

    // åŠ è½½è¾“å…¥æ•°æ®åˆ°GPU
    bool LoadInput(const std::string& inputFile) {
        std::cout << "Loading input file: " << inputFile << std::endl;

        std::ifstream input(inputFile, std::ios::binary);
        if (!input) {
            std::cerr << "ERROR: Cannot open input file: " << inputFile << std::endl;
            return false;
        }

        // ç›´æ¥è¯»å–FP16æ•°æ® (Batch=16, FP16æ ¼å¼)
        size_t inputSize = INPUT_N * INPUT_C * INPUT_H * INPUT_W * sizeof(uint16_t);  // FP16 = 2 bytes
        std::vector<uint16_t> hostInputFP16(INPUT_N * INPUT_C * INPUT_H * INPUT_W);
        input.read(reinterpret_cast<char*>(hostInputFP16.data()), inputSize);
        input.close();

        // ç›´æ¥ä»CPUæ‹·è´FP16æ•°æ®åˆ°GPU
        CHECK_CUDA(cudaMemcpy(inputBuffer, hostInputFP16.data(), inputSize, cudaMemcpyHostToDevice));

        // è°ƒè¯•ï¼šæ‰“å°è¾“å…¥æ•°æ®ä¿¡æ¯
        std::cout << "Input data loaded successfully! (Batch=" << INPUT_N << ", FP16)" << std::endl;
        std::cout << "FP16 input size: " << inputSize << " bytes" << std::endl;
        
        // æ£€æŸ¥ä¸åŒæ ·æœ¬çš„è¾“å…¥æ•°æ®æ˜¯å¦ç›¸åŒ
        std::cout << "Checking input data consistency..." << std::endl;
        bool all_samples_same = true;
        size_t sample_size = INPUT_C * INPUT_H * INPUT_W;
        
        for (int batch_idx = 1; batch_idx < INPUT_N; ++batch_idx) {
            for (size_t i = 0; i < sample_size; ++i) {
                if (hostInputFP16[i] != hostInputFP16[batch_idx * sample_size + i]) {
                    all_samples_same = false;
                    break;
                }
            }
            if (!all_samples_same) break;
        }
        
        if (all_samples_same) {
            std::cout << "âš ï¸ è­¦å‘Š: æ‰€æœ‰æ ·æœ¬çš„è¾“å…¥æ•°æ®ç›¸åŒ (è¿™æ˜¯é¢„æœŸçš„ï¼Œå› ä¸ºä½¿ç”¨åŒä¸€å¼ å›¾ç‰‡)" << std::endl;
        } else {
            std::cout << "âœ… ä¸åŒæ ·æœ¬çš„è¾“å…¥æ•°æ®ä¸åŒ" << std::endl;
        }
        
        // æ‰“å°å‰10ä¸ªè¾“å…¥å€¼ (FP16)
        std::cout << "First 10 input values (FP16):" << std::endl;
        for (size_t i = 0; i < 10 && i < hostInputFP16.size(); ++i) {
            std::cout << "  [" << i << "] FP16: " << hostInputFP16[i] << std::endl;
        }
        
        // æ˜¾ç¤ºæ•°å€¼èŒƒå›´
        uint16_t min_val = *std::min_element(hostInputFP16.begin(), hostInputFP16.end());
        uint16_t max_val = *std::max_element(hostInputFP16.begin(), hostInputFP16.end());
        std::cout << "Input value range: [" << min_val << ", " << max_val << "]" << std::endl;
        return true;
    }
};

int main(int argc, char* argv[]) {
    std::cout << "=== ResNet50 TensorRT FP16æ¨ç† + TOPSæ€§èƒ½åˆ†æ (Batch=16, FP16) ===" << std::endl;

    if (argc < 3) {
        std::cerr << "Usage: " << argv[0] << " <weights_file> <input_file> [output_file]" << std::endl;
        std::cerr << "Example: " << argv[0] << " resnet50-tensorrt-fp16.wts input_fp16_batch16_nchw.bin output.bin" << std::endl;
        std::cerr << "Note: æœ¬ç¨‹åºä»…æ”¯æŒ Batch=16 çš„ FP16 æ¨ç†" << std::endl;
        return 1;
    }

    std::string weightsFile = argv[1];
    std::string inputFile = argv[2];
    std::string outputFile = (argc > 3) ? argv[3] : "output.bin";

    try {
        // åˆ›å»ºResNet50å®ä¾‹ (Batch=16)
        std::cout << "\nâœ… åˆå§‹åŒ–FP16æ¨ç†å¼•æ“ (Batch=16)" << std::endl;
        ResNet50TensorRT resnet50;

        // åŠ è½½æƒé‡
        std::cout << "\nğŸ“ åŠ è½½FP16æƒé‡æ–‡ä»¶..." << std::endl;
        resnet50.LoadWeights(weightsFile);
        std::cout << "âœ… æƒé‡åŠ è½½å®Œæˆ!" << std::endl;

        // æ„å»ºç½‘ç»œ
        std::cout << "\nğŸ”§ æ„å»º FP16 ResNet50 ç½‘ç»œ..." << std::endl;
        resnet50.BuildResNet50();
        std::cout << "âœ… FP16ç½‘ç»œæ„å»ºå®Œæˆ!" << std::endl;

        // æ„å»ºå¼•æ“
        std::cout << "\nâš™ï¸  æ„å»ºTensorRTå¼•æ“..." << std::endl;
        resnet50.BuildEngine();
        std::cout << "âœ… TensorRTå¼•æ“æ„å»ºå®Œæˆ!" << std::endl;

        // åˆ†é…å†…å­˜
        std::cout << "\nğŸ’¾ åˆ†é…GPUå†…å­˜..." << std::endl;
        resnet50.AllocateBuffers();
        std::cout << "âœ… GPUå†…å­˜åˆ†é…å®Œæˆ!" << std::endl;

        // åŠ è½½è¾“å…¥æ•°æ®
        std::cout << "\nğŸ“¥ åŠ è½½è¾“å…¥æ–‡ä»¶: " << inputFile << std::endl;
        if (!resnet50.LoadInput(inputFile)) {
            std::cerr << "âŒ è¾“å…¥åŠ è½½å¤±è´¥" << std::endl;
            return 1;
        }

        // æ‰§è¡Œå•æ¬¡æ¨ç†éªŒè¯
        std::cout << "\nğŸ” æ‰§è¡Œå•æ¬¡æ¨ç†éªŒè¯..." << std::endl;
        std::vector<float> output;
        if (!resnet50.DoInference(output)) {
            std::cerr << "âŒ æ¨ç†æ‰§è¡Œå¤±è´¥" << std::endl;
            return 1;
        }

        // æ‰“å°æ¨ç†ç»“æœ
        std::cout << "\n=== æ¨ç†ç»“æœéªŒè¯ ===" << std::endl;
        resnet50.PrintResults(output);

        // ä¿å­˜è¾“å‡ºåˆ°æ–‡ä»¶
        std::ofstream outFile(outputFile, std::ios::binary);
        if (outFile) {
            outFile.write(reinterpret_cast<char*>(output.data()), output.size() * sizeof(float));
            outFile.close();
            std::cout << "\nâœ… è¾“å‡ºå·²ä¿å­˜åˆ°: " << outputFile << std::endl;
        }

        // æ‰§è¡Œæ€§èƒ½æµ‹è¯•å’ŒTOPSè®¡ç®—
        std::cout << "\nğŸš€ å¼€å§‹æ€§èƒ½æµ‹è¯•å’ŒTOPSåˆ†æ..." << std::endl;
        if (!resnet50.DoPerformanceTest(100, 10)) {  // 100æ¬¡æµ‹è¯•ï¼Œ10æ¬¡é¢„çƒ­
            std::cerr << "âŒ æ€§èƒ½æµ‹è¯•å¤±è´¥" << std::endl;
            return 1;
        }

        // æ˜¾ç¤ºGPUä¿¡æ¯
        std::cout << "\n=== ç¡¬ä»¶ä¿¡æ¯ ===" << std::endl;
        cudaDeviceProp prop;
        if (cudaGetDeviceProperties(&prop, 0) == cudaSuccess) {
            std::cout << "GPUè®¾å¤‡: " << prop.name << std::endl;
            std::cout << "è®¡ç®—èƒ½åŠ›: " << prop.major << "." << prop.minor << std::endl;
            std::cout << "æ˜¾å­˜å¤§å°: " << (prop.totalGlobalMem / (1024 * 1024)) << " MB" << std::endl;
            std::cout << "å¤šå¤„ç†å™¨æ•°é‡: " << prop.multiProcessorCount << std::endl;
            std::cout << "æœ€å¤§çº¿ç¨‹æ•°: " << prop.maxThreadsPerBlock << std::endl;
        } else {
            std::cout << "âš ï¸ æ— æ³•è·å–GPUä¿¡æ¯" << std::endl;
        }

        std::cout << "\nğŸ‰ ResNet50 FP16æ¨ç† + TOPSåˆ†æå®Œæˆ! (Batch=16, FP16)" << std::endl;
        std::cout << "ğŸ“Š æ€§èƒ½åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆï¼Œè¯·æŸ¥çœ‹ä¸Šè¿°TOPSè®¡ç®—ç»“æœ" << std::endl;

    } catch (const std::exception& e) {
        std::cerr << "ERROR: " << e.what() << std::endl;
        return 1;
    }

    return 0;
}
